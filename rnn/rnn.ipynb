{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/rn3.png\" width=\"200\">\n",
    "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Redes recurrentes, implementación simple\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 9 de mayo de 2018.\n",
    "\n",
    "En esta libreta vamos a explorar como desarrollar redes recurrentes, simples y basadas en unidades de memoria de largo y corto plazo (LSTM), aplicadas a la generación automática de texto.\n",
    "\n",
    "Dado que estamos en México en año electoral, y que se vota por diputados y presidentes municipales en muchisimos municipios del país, nos preguntamos si podríamos inventar nuevos municipios para que todos los candidatos tuvieran un lugar que gobernar. Así, generamos una lista con el nombre de todos los municipios de México, y la vamos a usar para aprender los nombres, y generar nombres a partir de una red recurrente. Esto es interesante ya que en México hay muchos municipios cuyos nombres tienen raices del español, el nahuatl, direrentes lenguas mayas, varias lenguas de la familia yuto-azteca, e inclusive algunos que son palabras inventadas (como Mexicali). Así que generar nombres de municipios mexicanos *creibles* es un problema interesante.\n",
    "\n",
    "El archivo con el nombre de los municipios se encuentra para su descarga [aqui](../varios/municipios.txt).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Redes recurrentes: Desarrollar una red recurrente completamente a pie.\n",
    "\n",
    "Con el fin de entender como funcionan las redes neuronales, vamos a aplicar un modelo de generaci´pn de texto *letra a letra*, en el cual tanto la arquitectura como el método de aprendizaje sean programados a mano. \n",
    "\n",
    "No vamos a pedir que lo programen todo solos, simplemente que utilicen el modelo programados en este [gist](https://gist.github.com/karpathy/d4dee566867f8291f086), y lo adapten a leer el nombre de los municipios y a generar nombres de municipios.\n",
    "\n",
    "Para esto:\n",
    "\n",
    "1. Copiiar el contenido del *gist* y comentarlo en español (y cambiar algo de código de forma que quede mñas claro para ti y para mi).\n",
    "\n",
    "2. Copiar y comentar en español el contenido del método de verificción de gradiente (para limpiar el código de errores) y usarlo por unas cuantas iteraciones para demostrar que el algoritmo de entrenamiento funciona correctamente.\n",
    "\n",
    "3. Ajustar los hiperparámetros del modelo, así como los parámetros del algoritmo de entrenamiento con el fin de generar una lista de nombres de municipios creibles, pero sin sobreaprendizaje (esto es, que copie vilmente el nombre de municipios existentes). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh( Wxh@xs[t] + Whh@hs[t-1] + bh) # hidden state\n",
    "        ys[t] = Why@hs[t] + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += dy@hs[t].T\n",
    "        dby += dy\n",
    "        dh = Why.T@dy + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += dhraw@xs[t].T\n",
    "        dWhh += dhraw@hs[t-1].T\n",
    "        dhnext = Whh.T@dhraw\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(Wxh@x + Whh@h + bh)\n",
    "        y = Why@h + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "def gradCheck(inputs, targets, hprev):\n",
    "    global Wxh, Whh, Why, bh, by\n",
    "    num_checks, delta = 10, 1e-5\n",
    "    _, dWxh, dWhh, dWhy, dbh, dby, _ = lossFun(inputs, targets, hprev)\n",
    "    for param,dparam,name in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], ['Wxh', 'Whh', 'Why', 'bh', 'by']):\n",
    "        s0 = dparam.shape\n",
    "        s1 = param.shape\n",
    "        assert s0 == s1, ('Error dims dont match: %s and %s.' % ('s0', 's1'))\n",
    "        print (name)\n",
    "        for i in range(num_checks):\n",
    "            ri = int(uniform(0,param.size))\n",
    "            # evaluate cost at [x + delta] and [x - delta]\n",
    "            old_val = param.flat[ri]\n",
    "            param.flat[ri] = old_val + delta\n",
    "            cg0, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "            param.flat[ri] = old_val - delta\n",
    "            cg1, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "            param.flat[ri] = old_val # reset old value for this parameter\n",
    "            # fetch both numerical and analytic gradient\n",
    "            grad_analytic = dparam.flat[ri]\n",
    "            grad_numerical = (cg0 - cg1) / ( 2 * delta )\n",
    "            rel_error = abs(grad_analytic - grad_numerical) / (abs(grad_numerical + grad_analytic) + np.spacing(1))\n",
    "            print ('%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error))\n",
    "            # rel_error should be on order of 1e-7 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 35529 characters, 68 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('municipios.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000002, 0.000002 => 5.199111e-05 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 1.199004e-04 \n",
      "Whh\n",
      "-0.000083, -0.000083 => 2.738555e-06 \n",
      "0.032185, 0.032185 => 1.843754e-08 \n",
      "-0.000095, -0.000095 => 3.252871e-06 \n",
      "0.205091, 0.205091 => 1.881212e-09 \n",
      "0.021586, 0.021586 => 8.055897e-09 \n",
      "0.000180, 0.000180 => 1.647701e-06 \n",
      "0.000056, 0.000056 => 1.225902e-06 \n",
      "0.004727, 0.004727 => 3.620265e-08 \n",
      "0.000003, 0.000003 => 1.510121e-04 \n",
      "0.104872, 0.104872 => 4.866652e-10 \n",
      "Why\n",
      "0.018847, 0.018847 => 1.198513e-08 \n",
      "0.008293, 0.008293 => 2.086122e-08 \n",
      "-0.018783, -0.018783 => 1.066953e-08 \n",
      "-0.082043, -0.082043 => 2.668276e-10 \n",
      "0.006762, 0.006762 => 2.267309e-08 \n",
      "0.018666, 0.018666 => 1.776311e-09 \n",
      "-0.003571, -0.003571 => 5.557663e-09 \n",
      "-0.061278, -0.061278 => 4.173541e-09 \n",
      "-0.010793, -0.010793 => 4.600870e-08 \n",
      "-0.010714, -0.010714 => 5.371750e-08 \n",
      "bh\n",
      "0.124035, 0.124035 => 2.435685e-09 \n",
      "0.035849, 0.035849 => 4.880123e-09 \n",
      "0.111418, 0.111418 => 1.677746e-09 \n",
      "0.000493, 0.000493 => 4.360075e-07 \n",
      "1.026826, 1.026826 => 8.121355e-11 \n",
      "-0.000017, -0.000017 => 5.355316e-06 \n",
      "-0.000819, -0.000819 => 2.928503e-07 \n",
      "-0.000004, -0.000004 => 5.315586e-05 \n",
      "-0.000065, -0.000065 => 7.235154e-06 \n",
      "0.000493, 0.000493 => 4.360075e-07 \n",
      "by\n",
      "-0.883093, -0.883093 => 1.250976e-10 \n",
      "-1.204838, -1.204838 => 4.964110e-11 \n",
      "0.015570, 0.015570 => 1.209393e-08 \n",
      "0.021099, 0.021099 => 8.597406e-09 \n",
      "0.098595, 0.098595 => 1.551653e-09 \n",
      "0.060408, 0.060408 => 3.946240e-09 \n",
      "0.113268, 0.113268 => 1.669870e-09 \n",
      "0.161565, 0.161565 => 8.072626e-10 \n",
      "0.025254, 0.025254 => 5.892887e-09 \n",
      "-2.500102, -2.500102 => 1.731509e-10 \n",
      "----\n",
      " \n",
      "Morazari\n",
      "Levaso\n",
      "duá ocemta Ra Esca\n",
      "Siz íal\n",
      "Micas\n",
      "Jexatda\n",
      "Mane\n",
      "Lpas\n",
      "Mo\n",
      "Hua\n",
      "Jíl\n",
      "Mamata MSa\n",
      "MJuHua do ej Varia Lárual GJeulmepattMaría\n",
      "Lootán\n",
      "Doosdepetquas\n",
      "Le Cuuba\n",
      "Mahcanán\n",
      "Men\n",
      "MpacimapNal\n",
      "Mance\n",
      "Ma\n",
      "Ila \n",
      "----\n",
      "iter 500, loss: 92.640303\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "0.184152, 0.184152 => 2.297753e-09 \n",
      "-0.080188, -0.080188 => 4.445687e-10 \n",
      "0.003476, 0.003476 => 2.193453e-07 \n",
      "0.000678, 0.000678 => 2.497659e-07 \n",
      "-0.827740, -0.827740 => 5.148627e-10 \n",
      "-0.002041, -0.002041 => 2.390605e-07 \n",
      "-0.066755, -0.066755 => 7.102307e-09 \n",
      "-0.000286, -0.000286 => 1.760854e-07 \n",
      "-0.000128, -0.000128 => 4.285244e-06 \n",
      "0.000537, 0.000537 => 1.087596e-07 \n",
      "Why\n",
      "-0.033075, -0.033075 => 5.281213e-09 \n",
      "-1.171711, -1.171711 => 3.390527e-10 \n",
      "-0.024203, -0.024203 => 1.437631e-08 \n",
      "0.003835, 0.003835 => 7.315135e-08 \n",
      "0.001151, 0.001151 => 1.433092e-07 \n",
      "-0.406262, -0.406262 => 3.153795e-10 \n",
      "0.001459, 0.001459 => 3.184022e-08 \n",
      "0.028092, 0.028092 => 6.367824e-10 \n",
      "0.005662, 0.005662 => 2.424687e-08 \n",
      "0.002967, 0.002967 => 4.135283e-08 \n",
      "bh\n",
      "0.232360, 0.232360 => 1.457813e-09 \n",
      "0.260196, 0.260196 => 2.260158e-09 \n",
      "-0.000834, -0.000834 => 2.576038e-07 \n",
      "0.012634, 0.012634 => 3.102375e-08 \n",
      "0.007822, 0.007822 => 1.292426e-08 \n",
      "0.000176, 0.000176 => 7.711196e-07 \n",
      "0.000096, 0.000096 => 6.926562e-07 \n",
      "-0.070284, -0.070284 => 5.176945e-09 \n",
      "0.033637, 0.033637 => 2.439618e-08 \n",
      "-0.088603, -0.088603 => 2.620848e-09 \n",
      "by\n",
      "-0.557132, -0.557132 => 2.852764e-10 \n",
      "0.020655, 0.020655 => 1.753132e-09 \n",
      "0.011801, 0.011801 => 1.429581e-08 \n",
      "0.091378, 0.091378 => 1.863528e-09 \n",
      "0.056016, 0.056016 => 1.903857e-09 \n",
      "0.004357, 0.004357 => 4.675553e-08 \n",
      "0.536617, 0.536617 => 1.226821e-10 \n",
      "0.524344, 0.524344 => 1.036532e-09 \n",
      "0.267115, 0.267115 => 2.167032e-09 \n",
      "0.524344, 0.524344 => 1.036532e-09 \n",
      "----\n",
      " Noüderera\n",
      "San Matcifrertcaauten arrti Sotagí drPimax Apánterz Mepie Hixte\n",
      "Sas\n",
      "Sa Goxa\n",
      "Sanvatlas Sanotisa\n",
      "Sarao drina\n",
      "Sjori Cagu\n",
      "Sentl Lueta\n",
      "Sancenayani\n",
      "Santla\n",
      "Sati daytistlirrín Men Rarin\n",
      "Sarpimic\n",
      "San \n",
      "----\n",
      "iter 1000, loss: 79.105613\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 1.574759e-01 \n",
      "0.000002, 0.000002 => 9.668352e-05 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000017, 0.000017 => 4.223763e-06 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "-0.011228, -0.011228 => 5.772056e-08 \n",
      "0.448231, 0.448231 => 8.134156e-11 \n",
      "-0.011536, -0.011536 => 2.931418e-08 \n",
      "-0.291710, -0.291710 => 9.581094e-10 \n",
      "-0.008683, -0.008683 => 5.057777e-08 \n",
      "0.032834, 0.032834 => 6.965447e-09 \n",
      "0.011241, 0.011241 => 6.067976e-09 \n",
      "-0.000018, -0.000018 => 1.557469e-05 \n",
      "0.081816, 0.081816 => 4.651814e-09 \n",
      "0.002140, 0.002140 => 2.361880e-07 \n",
      "Why\n",
      "-0.002175, -0.002175 => 1.094327e-07 \n",
      "-0.010075, -0.010075 => 3.857853e-08 \n",
      "0.001031, 0.001031 => 5.053837e-07 \n",
      "0.003853, 0.003853 => 6.809856e-08 \n",
      "0.009661, 0.009661 => 2.739830e-08 \n",
      "-0.073094, -0.073094 => 1.799833e-09 \n",
      "-0.001403, -0.001403 => 3.611901e-07 \n",
      "0.081020, 0.081020 => 4.505114e-10 \n",
      "0.883576, 0.883576 => 3.471287e-10 \n",
      "0.091805, 0.091805 => 1.220163e-09 \n",
      "bh\n",
      "0.158995, 0.158995 => 2.865610e-09 \n",
      "-0.000039, -0.000039 => 7.907671e-06 \n",
      "0.003444, 0.003444 => 6.291935e-09 \n",
      "0.027485, 0.027485 => 9.609997e-09 \n",
      "-0.014374, -0.014374 => 3.681031e-08 \n",
      "0.007300, 0.007300 => 3.812555e-09 \n",
      "0.302583, 0.302583 => 5.979751e-10 \n",
      "0.002136, 0.002136 => 2.563571e-07 \n",
      "0.002175, 0.002175 => 1.238160e-07 \n",
      "0.001145, 0.001145 => 3.772132e-08 \n",
      "by\n",
      "0.001740, 0.001740 => 1.993171e-08 \n",
      "0.001753, 0.001753 => 2.704861e-08 \n",
      "0.023088, 0.023088 => 3.861049e-09 \n",
      "0.011613, 0.011613 => 4.864405e-09 \n",
      "-0.267626, -0.267626 => 8.135228e-10 \n",
      "0.007775, 0.007775 => 8.144040e-08 \n",
      "-2.109665, -2.109665 => 1.545674e-10 \n",
      "0.007775, 0.007775 => 8.144040e-08 \n",
      "0.014121, 0.014121 => 4.007459e-09 \n",
      "0.066402, 0.066402 => 1.827042e-09 \n",
      "----\n",
      " capal Arotlapa\n",
      "Atincahtandaama\n",
      "Tettoca\n",
      "Attin\n",
      "Amanbajityajate\n",
      "Am\n",
      "Yoniatlán\n",
      "Ahukirralalde de Hoxtla\n",
      "Asloán\n",
      "Ati\n",
      "Ahuilziugu\n",
      "Zingucipatatla ReboltilVueAconatiltorarutl Aristlagila\n",
      "Ahuiscapagonxa\n",
      "Atanálolan \n",
      "----\n",
      "iter 1500, loss: 70.622972\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.001861, 0.001861 => 3.019942e-07 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "0.000166, 0.000166 => 5.364066e-07 \n",
      "-0.000043, -0.000043 => 7.956773e-06 \n",
      "0.000443, 0.000443 => 5.698771e-07 \n",
      "-0.030234, -0.030234 => 1.120008e-08 \n",
      "0.072184, 0.072184 => 1.975420e-10 \n",
      "-0.000211, -0.000211 => 2.639516e-06 \n",
      "-0.000052, -0.000052 => 9.184122e-07 \n",
      "-0.076972, -0.076972 => 4.584305e-09 \n",
      "-0.000684, -0.000684 => 3.802072e-07 \n",
      "-0.002150, -0.002150 => 6.019382e-08 \n",
      "Why\n",
      "0.000869, 0.000869 => 1.502407e-07 \n",
      "0.026991, 0.026991 => 1.713058e-08 \n",
      "1.399903, 1.399903 => 2.080409e-10 \n",
      "-2.129887, -2.129887 => 1.764545e-10 \n",
      "0.030549, 0.030549 => 1.174230e-08 \n",
      "-0.002583, -0.002583 => 3.080839e-08 \n",
      "0.001333, 0.001333 => 2.652705e-09 \n",
      "-0.001091, -0.001091 => 8.733575e-07 \n",
      "1.032838, 1.032838 => 6.634261e-11 \n",
      "0.083968, 0.083968 => 5.234972e-09 \n",
      "bh\n",
      "-2.644431, -2.644431 => 2.073009e-10 \n",
      "0.030502, 0.030502 => 7.468818e-09 \n",
      "-0.004793, -0.004793 => 3.436679e-08 \n",
      "-0.041020, -0.041020 => 1.573478e-10 \n",
      "-0.000000, -0.000000 => 6.390012e-04 \n",
      "0.002205, 0.002205 => 3.026578e-07 \n",
      "0.704459, 0.704459 => 5.891979e-11 \n",
      "0.356182, 0.356182 => 1.374578e-09 \n",
      "-0.499265, -0.499265 => 2.588955e-10 \n",
      "0.000030, 0.000030 => 1.309569e-05 \n",
      "by\n",
      "-0.923667, -0.923667 => 4.591405e-10 \n",
      "0.004246, 0.004246 => 5.918448e-08 \n",
      "-3.384493, -3.384493 => 4.891880e-12 \n",
      "0.006338, 0.006338 => 3.338688e-08 \n",
      "0.020553, 0.020553 => 7.144640e-09 \n",
      "0.009301, 0.009301 => 2.102720e-08 \n",
      "0.438498, 0.438498 => 1.401220e-09 \n",
      "0.244626, 0.244626 => 2.980715e-09 \n",
      "0.028913, 0.028913 => 5.335411e-09 \n",
      "1.857882, 1.857882 => 3.547397e-10 \n",
      "----\n",
      " vos\n",
      "Ouil\n",
      "Oscos\n",
      "Otlaálamabe\n",
      "Oxoro\n",
      "Opan\n",
      "Jes de FKas\n",
      "Jilasos Onroñinvoc as de Deve\n",
      "Otitirpepa\n",
      "gav8el\n",
      "Satlgon\n",
      "Milán\n",
      "Oro\n",
      "Oc\n",
      "Alántpilán\n",
      "Xiayacros\n",
      "Lapan\n",
      "Odani\n",
      "Otlán\n",
      "de Na\n",
      "Olare\n",
      "Ozo\n",
      "Mésan\n",
      "Ll Zumandacofo\n",
      "Ocá\n",
      "E \n",
      "----\n",
      "iter 2000, loss: 66.944301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GradCheck\n",
      "Wxh\n",
      "-0.000025, -0.000025 => 3.049486e-07 \n",
      "-0.000191, -0.000191 => 9.811356e-07 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.243554, 0.243554 => 1.568564e-09 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "-0.000104, -0.000104 => 8.685188e-07 \n",
      "0.010965, 0.010965 => 7.642658e-09 \n",
      "0.724432, 0.724432 => 5.837095e-10 \n",
      "-0.032015, -0.032015 => 8.226177e-09 \n",
      "0.161073, 0.161073 => 1.146864e-09 \n",
      "0.319333, 0.319333 => 1.853579e-10 \n",
      "-0.001284, -0.001284 => 2.247995e-08 \n",
      "0.002948, 0.002948 => 1.863920e-08 \n",
      "0.404073, 0.404073 => 1.063112e-09 \n",
      "-0.218223, -0.218223 => 6.381668e-10 \n",
      "Why\n",
      "-0.028153, -0.028153 => 8.897650e-09 \n",
      "0.000997, 0.000997 => 2.262940e-07 \n",
      "0.329917, 0.329917 => 3.047270e-10 \n",
      "-0.012524, -0.012524 => 1.031086e-08 \n",
      "0.019214, 0.019214 => 1.270331e-08 \n",
      "0.005228, 0.005228 => 8.971013e-09 \n",
      "-0.008030, -0.008030 => 8.519714e-09 \n",
      "0.023757, 0.023757 => 1.378897e-08 \n",
      "-1.097955, -1.097955 => 2.222295e-10 \n",
      "-0.004017, -0.004017 => 5.705773e-09 \n",
      "bh\n",
      "0.031722, 0.031722 => 1.089821e-08 \n",
      "-0.000001, -0.000001 => 4.635031e-04 \n",
      "0.003046, 0.003046 => 1.354866e-08 \n",
      "-0.000046, -0.000046 => 7.391689e-06 \n",
      "-0.011423, -0.011423 => 3.313977e-08 \n",
      "-0.269453, -0.269453 => 2.458203e-09 \n",
      "0.001124, 0.001124 => 1.101332e-07 \n",
      "0.001505, 0.001505 => 1.055955e-07 \n",
      "-0.000046, -0.000046 => 7.391689e-06 \n",
      "0.000154, 0.000154 => 4.854188e-07 \n",
      "by\n",
      "0.001971, 0.001971 => 1.903535e-07 \n",
      "0.299939, 0.299939 => 1.947582e-09 \n",
      "0.137775, 0.137775 => 4.959643e-10 \n",
      "0.000707, 0.000707 => 7.007219e-09 \n",
      "-0.610867, -0.610867 => 2.914648e-10 \n",
      "-0.881725, -0.881725 => 1.889346e-11 \n",
      "-0.121668, -0.121668 => 2.281983e-09 \n",
      "0.064344, 0.064344 => 5.796469e-10 \n",
      "0.003272, 0.003272 => 7.119335e-08 \n",
      "0.299939, 0.299939 => 1.947582e-09 \n",
      "----\n",
      " oreca\n",
      "Saz\n",
      "Sapacasa\n",
      "Satcal\n",
      "Santa Cacaríohita Cortl\n",
      "Santatta Zaxta Huadeteyucal Pali duán M.ñíela\n",
      "San\n",
      "Santica\n",
      "Sán Aulía Cadue Guareca\n",
      "Sas GarSanthag\n",
      "Sánla Yuarmede\n",
      "Saz Joría Miaguiapo Moantaquitunicá\n",
      "Sa \n",
      "----\n",
      "iter 2500, loss: 60.399843\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000001, 0.000001 => 5.787979e-05 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000559, 0.000559 => 8.595694e-07 \n",
      "Whh\n",
      "-0.744079, -0.744079 => 5.552408e-10 \n",
      "-0.080875, -0.080875 => 1.850594e-09 \n",
      "0.010627, 0.010627 => 7.342934e-08 \n",
      "0.039992, 0.039992 => 9.842827e-09 \n",
      "0.017832, 0.017832 => 3.816531e-10 \n",
      "-0.002101, -0.002101 => 2.421063e-07 \n",
      "-0.000022, -0.000022 => 1.627349e-05 \n",
      "0.019112, 0.019112 => 1.417205e-08 \n",
      "-0.036270, -0.036270 => 7.972079e-09 \n",
      "-0.000309, -0.000309 => 2.598571e-07 \n",
      "Why\n",
      "0.175393, 0.175393 => 2.687511e-10 \n",
      "-0.031913, -0.031913 => 1.883173e-09 \n",
      "0.004438, 0.004438 => 2.341010e-08 \n",
      "-0.000326, -0.000326 => 5.494682e-07 \n",
      "0.013002, 0.013002 => 2.092593e-08 \n",
      "0.063267, 0.063267 => 7.775584e-10 \n",
      "0.001044, 0.001044 => 7.295477e-08 \n",
      "0.001650, 0.001650 => 1.541369e-07 \n",
      "0.000297, 0.000297 => 8.340579e-07 \n",
      "0.002615, 0.002615 => 4.582427e-08 \n",
      "bh\n",
      "0.002075, 0.002075 => 2.328627e-07 \n",
      "-0.460511, -0.460511 => 1.042245e-09 \n",
      "0.697383, 0.697383 => 2.762344e-10 \n",
      "-0.604859, -0.604859 => 4.766232e-11 \n",
      "0.009841, 0.009841 => 5.595782e-08 \n",
      "0.009703, 0.009703 => 5.112366e-08 \n",
      "-0.010139, -0.010139 => 4.140761e-08 \n",
      "-0.000017, -0.000017 => 2.570457e-05 \n",
      "0.000024, 0.000024 => 1.395065e-05 \n",
      "0.693675, 0.693675 => 2.401643e-10 \n",
      "by\n",
      "0.001552, 0.001552 => 1.308078e-07 \n",
      "0.009420, 0.009420 => 2.528149e-08 \n",
      "0.139274, 0.139274 => 3.463159e-10 \n",
      "0.011389, 0.011389 => 2.835479e-09 \n",
      "0.005558, 0.005558 => 1.876209e-08 \n",
      "0.024400, 0.024400 => 1.163661e-08 \n",
      "0.011168, 0.011168 => 1.268402e-08 \n",
      "-0.433358, -0.433358 => 7.660706e-10 \n",
      "0.012101, 0.012101 => 1.073654e-09 \n",
      "0.077917, 0.077917 => 4.184999e-09 \n",
      "----\n",
      " avarla\n",
      "Chitán\n",
      "Caco\n",
      "Casion\n",
      "Carpen\n",
      "Cos\n",
      "Camoriocalon\n",
      "Amtajeb aro\n",
      "Chogha\n",
      "Ascortlán derio\n",
      "Chi Sianatermames\n",
      "Zexiiy\n",
      "Chuilra\n",
      "Zán Omel\n",
      "Cadro Chepantellen\n",
      "Cac\n",
      "Chumarco\n",
      "Canre\n",
      "Nez\n",
      "Calgo\n",
      "Caltitaco\n",
      "Chanacad\n",
      "Chimrl \n",
      "----\n",
      "iter 3000, loss: 58.474155\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000130, 0.000130 => 1.914272e-06 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "-0.003283, -0.003283 => 1.157022e-07 \n",
      "-0.017856, -0.017856 => 1.460399e-08 \n",
      "0.000140, 0.000140 => 1.525490e-06 \n",
      "-0.004764, -0.004764 => 5.157142e-08 \n",
      "-0.000551, -0.000551 => 1.553199e-07 \n",
      "0.005210, 0.005210 => 3.524869e-08 \n",
      "-0.010118, -0.010118 => 1.008880e-08 \n",
      "0.003968, 0.003968 => 7.687400e-08 \n",
      "-0.000048, -0.000048 => 5.482254e-06 \n",
      "-0.000149, -0.000149 => 1.656381e-06 \n",
      "Why\n",
      "-0.023047, -0.023047 => 2.819386e-09 \n",
      "-0.116386, -0.116386 => 1.576383e-09 \n",
      "0.961576, 0.961576 => 1.184086e-10 \n",
      "-0.081868, -0.081868 => 8.505763e-10 \n",
      "-0.890718, -0.890718 => 1.273942e-10 \n",
      "0.000273, 0.000273 => 2.167239e-06 \n",
      "-1.510737, -1.510737 => 2.770009e-10 \n",
      "-0.004470, -0.004470 => 9.198351e-08 \n",
      "1.008586, 1.008586 => 8.539791e-11 \n",
      "-0.390436, -0.390436 => 1.977646e-10 \n",
      "bh\n",
      "0.000252, 0.000252 => 9.703501e-07 \n",
      "-0.001453, -0.001453 => 6.405135e-08 \n",
      "-0.000001, -0.000001 => 6.345666e-05 \n",
      "-0.447809, -0.447809 => 1.675790e-10 \n",
      "0.080891, 0.080891 => 6.001963e-09 \n",
      "-0.015387, -0.015387 => 1.977744e-08 \n",
      "-0.001180, -0.001180 => 1.230262e-08 \n",
      "-0.171310, -0.171310 => 1.137478e-10 \n",
      "-0.000290, -0.000290 => 6.076553e-07 \n",
      "-0.488890, -0.488890 => 8.240300e-10 \n",
      "by\n",
      "-0.905395, -0.905395 => 3.763157e-10 \n",
      "-0.905395, -0.905395 => 3.763157e-10 \n",
      "0.108648, 0.108648 => 2.373231e-09 \n",
      "0.148211, 0.148211 => 7.020449e-10 \n",
      "0.027137, 0.027137 => 1.631862e-08 \n",
      "0.120144, 0.120144 => 7.407396e-10 \n",
      "0.001868, 0.001868 => 1.182483e-07 \n",
      "-1.565571, -1.565571 => 9.318563e-11 \n",
      "0.001989, 0.001989 => 1.197579e-07 \n",
      "0.005221, 0.005221 => 8.635766e-08 \n",
      "----\n",
      " uano\n",
      "San\n",
      "Maguixbcuefaplitzun\n",
      "Pidaya Bram\n",
      "Sengo\n",
      "SanEs\n",
      "Nrildoma de de Ayancocba\n",
      "Saratzil Helanroján\n",
      "Sas\n",
      "San de de Cemos Tátemicuta de BeroóTtirautletaratey\n",
      "Tegranchicho Guengutotianasichaboco\n",
      "San Otíha  \n",
      "----\n",
      "iter 3500, loss: 58.839348\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "-0.000893, -0.000893 => 3.615946e-07 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000002, 0.000002 => 4.717124e-05 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "-0.092177, -0.092177 => 1.441714e-09 \n",
      "-0.009648, -0.009648 => 5.188829e-09 \n",
      "0.228319, 0.228319 => 1.092721e-09 \n",
      "-0.012023, -0.012023 => 3.299760e-08 \n",
      "0.000196, 0.000196 => 6.715951e-08 \n",
      "2.775580, 2.775580 => 4.749961e-12 \n",
      "-0.000739, -0.000739 => 4.907036e-07 \n",
      "0.000087, 0.000087 => 1.009624e-06 \n",
      "-0.002020, -0.002020 => 1.464590e-07 \n",
      "0.214772, 0.214772 => 3.639350e-09 \n",
      "Why\n",
      "-1.477398, -1.477398 => 1.423238e-10 \n",
      "-0.085216, -0.085216 => 2.133052e-09 \n",
      "0.000165, 0.000165 => 1.155094e-06 \n",
      "0.021969, 0.021969 => 1.068835e-08 \n",
      "0.014133, 0.014133 => 1.593750e-08 \n",
      "-0.000744, -0.000744 => 5.012096e-07 \n",
      "-0.072054, -0.072054 => 6.208093e-09 \n",
      "-0.955702, -0.955702 => 1.221276e-10 \n",
      "0.030583, 0.030583 => 7.957808e-09 \n",
      "0.799876, 0.799876 => 6.427213e-11 \n",
      "bh\n",
      "0.024375, 0.024375 => 6.035571e-09 \n",
      "0.016064, 0.016064 => 3.292980e-10 \n",
      "0.349736, 0.349736 => 5.267887e-10 \n",
      "0.068407, 0.068407 => 2.392982e-10 \n",
      "0.016064, 0.016064 => 3.292980e-10 \n",
      "0.011312, 0.011312 => 2.344842e-08 \n",
      "0.000109, 0.000109 => 7.622902e-06 \n",
      "-0.078026, -0.078026 => 8.051372e-09 \n",
      "-3.070919, -3.070919 => 3.052273e-10 \n",
      "-0.000950, -0.000950 => 7.681118e-07 \n",
      "by\n",
      "1.099917, 1.099917 => 4.089031e-10 \n",
      "0.011091, 0.011091 => 4.405762e-08 \n",
      "0.124711, 0.124711 => 4.069013e-11 \n",
      "0.009009, 0.009009 => 2.709147e-08 \n",
      "0.024311, 0.024311 => 7.960828e-09 \n",
      "0.005134, 0.005134 => 4.723252e-08 \n",
      "0.014118, 0.014118 => 1.725364e-08 \n",
      "0.016516, 0.016516 => 3.651716e-08 \n",
      "0.006610, 0.006610 => 8.205773e-08 \n",
      "0.002889, 0.002889 => 1.758185e-07 \n",
      "----\n",
      " costaley Nacostapas Astecotlá\n",
      "Sepa\n",
      "San Panungpaliso\n",
      "Sando Pedíe Sedro Tóyiolánto ManZoj\n",
      "Saneptéruita dezés Aumee Xoyetatiliare\n",
      "Sal InenchilE\n",
      "Sanlo Chuenta de Miéno gíapez\n",
      "San\n",
      "Santo Terta\n",
      "Sancheppanés\n",
      " \n",
      "----\n",
      "iter 4000, loss: 53.478756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "-0.000013, -0.000013 => 1.979532e-05 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "0.005127, 0.005127 => 1.069259e-07 \n",
      "0.079349, 0.079349 => 1.567505e-09 \n",
      "0.711987, 0.711987 => 1.924555e-10 \n",
      "0.000083, 0.000083 => 3.356439e-06 \n",
      "-0.000429, -0.000429 => 2.198174e-07 \n",
      "0.300727, 0.300727 => 8.052935e-10 \n",
      "0.705152, 0.705152 => 2.756064e-10 \n",
      "0.000121, 0.000121 => 8.595010e-06 \n",
      "-0.004021, -0.004021 => 6.636196e-08 \n",
      "-0.303639, -0.303639 => 4.702127e-10 \n",
      "Why\n",
      "-0.000745, -0.000745 => 4.046973e-07 \n",
      "0.809673, 0.809673 => 1.394931e-10 \n",
      "0.000170, 0.000170 => 1.389732e-06 \n",
      "0.000101, 0.000101 => 4.007529e-06 \n",
      "1.325711, 1.325711 => 1.850664e-10 \n",
      "-0.003522, -0.003522 => 5.033219e-09 \n",
      "-0.006512, -0.006512 => 3.834874e-09 \n",
      "-0.001503, -0.001503 => 1.158121e-07 \n",
      "0.003882, 0.003882 => 2.267426e-08 \n",
      "-0.116531, -0.116531 => 3.463288e-10 \n",
      "bh\n",
      "-0.685205, -0.685205 => 9.577137e-11 \n",
      "-0.443196, -0.443196 => 9.025772e-10 \n",
      "-0.334709, -0.334709 => 2.892559e-11 \n",
      "-0.053701, -0.053701 => 7.124587e-10 \n",
      "0.000048, 0.000048 => 4.896944e-06 \n",
      "-0.000326, -0.000326 => 2.139100e-07 \n",
      "-0.000483, -0.000483 => 3.818004e-07 \n",
      "-0.000001, -0.000001 => 5.537028e-04 \n",
      "-0.000435, -0.000435 => 1.130694e-07 \n",
      "-0.010168, -0.010168 => 5.401218e-08 \n",
      "by\n",
      "0.014799, 0.014799 => 2.155821e-08 \n",
      "0.003145, 0.003145 => 4.602461e-08 \n",
      "0.070988, 0.070988 => 4.495461e-09 \n",
      "0.438812, 0.438812 => 1.924637e-10 \n",
      "-2.322783, -2.322783 => 1.572935e-11 \n",
      "0.063630, 0.063630 => 6.116678e-09 \n",
      "-0.967918, -0.967918 => 2.266131e-10 \n",
      "0.154020, 0.154020 => 2.937916e-10 \n",
      "0.063899, 0.063899 => 2.849326e-09 \n",
      "0.063899, 0.063899 => 2.849326e-09 \n",
      "----\n",
      " lac\n",
      "Couchoh\n",
      "CuOcalacuvoda\n",
      "Comac\n",
      "Conla\n",
      "Conté\n",
      "Co Yerlo\n",
      "Cueucanta guapanac\n",
      "Solol da decuarcamas\n",
      "Camepalcoapic\n",
      "Corac\n",
      "Chues Caran\n",
      "Codez\n",
      "Cuillac\n",
      "Chuetuago Izalán\n",
      "Cazinde Rache\n",
      "Burozo\n",
      "Bopanto\n",
      "Cosrarhala de J \n",
      "----\n",
      "iter 4500, loss: 53.948493\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000826, 0.000826 => 1.088296e-07 \n",
      "-0.000000, -0.000000 => 2.819739e-01 \n",
      "0.000136, 0.000136 => 2.433821e-06 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "-0.000044, -0.000044 => 4.741634e-06 \n",
      "0.068648, 0.068648 => 6.114360e-09 \n",
      "-0.000032, -0.000032 => 3.164927e-06 \n",
      "-0.048630, -0.048630 => 3.292132e-09 \n",
      "-1.168899, -1.168899 => 1.979318e-10 \n",
      "0.000194, 0.000194 => 1.068962e-06 \n",
      "-0.051976, -0.051976 => 7.385368e-09 \n",
      "0.011477, 0.011477 => 1.021684e-08 \n",
      "-0.259269, -0.259269 => 1.256214e-09 \n",
      "0.011477, 0.011477 => 6.211224e-09 \n",
      "Why\n",
      "1.391281, 1.391281 => 9.775413e-12 \n",
      "-0.001234, -0.001234 => 4.172493e-07 \n",
      "-0.071436, -0.071436 => 2.351461e-09 \n",
      "-0.000547, -0.000547 => 1.515692e-07 \n",
      "-0.004911, -0.004911 => 9.383851e-08 \n",
      "-0.000040, -0.000040 => 6.869089e-06 \n",
      "-0.524634, -0.524634 => 3.697093e-11 \n",
      "-0.031539, -0.031539 => 1.054188e-08 \n",
      "0.136530, 0.136530 => 2.476429e-09 \n",
      "-2.497429, -2.497429 => 2.635529e-11 \n",
      "bh\n",
      "-0.003188, -0.003188 => 1.017515e-07 \n",
      "0.009000, 0.009000 => 2.346764e-08 \n",
      "-0.263541, -0.263541 => 4.532563e-10 \n",
      "0.362595, 0.362595 => 6.866740e-10 \n",
      "-0.000004, -0.000004 => 2.096520e-05 \n",
      "0.001012, 0.001012 => 1.038540e-07 \n",
      "-0.797914, -0.797914 => 4.100924e-10 \n",
      "-0.126813, -0.126813 => 1.596232e-09 \n",
      "-0.205625, -0.205625 => 1.975982e-09 \n",
      "0.002222, 0.002222 => 1.199263e-07 \n",
      "by\n",
      "0.336055, 0.336055 => 4.730651e-11 \n",
      "0.071306, 0.071306 => 1.663769e-09 \n",
      "0.013531, 0.013531 => 8.454981e-09 \n",
      "0.063936, 0.063936 => 1.005233e-09 \n",
      "0.010977, 0.010977 => 9.760893e-09 \n",
      "0.095586, 0.095586 => 7.723948e-10 \n",
      "0.070897, 0.070897 => 9.916368e-10 \n",
      "0.008216, 0.008216 => 3.091959e-08 \n",
      "0.000969, 0.000969 => 4.569237e-07 \n",
      "0.087440, 0.087440 => 7.567681e-10 \n",
      "----\n",
      " n\n",
      "San Teptla\n",
      "San Frante FCoruejha\n",
      "San JuesFéan Micil\n",
      "Pán Mello Jo Un\n",
      "San Frastiincadanno Hes\n",
      "Sanda\n",
      "San Fratohuima\n",
      "Santlán\n",
      "San Comextrres dam\n",
      "San Foginca Duesántencéngil Maric\n",
      "Quzal Rolcu\n",
      "Sagora Nuecco \n",
      "----\n",
      "iter 5000, loss: 54.888980\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "-0.000002, -0.000002 => 1.332439e-04 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "-0.000000, -0.000000 => 2.956953e-04 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "-0.000002, -0.000002 => 1.830962e-05 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.062531, 0.062531 => 6.497118e-09 \n",
      "Whh\n",
      "0.237503, 0.237503 => 1.526227e-09 \n",
      "0.373624, 0.373624 => 3.227557e-10 \n",
      "0.114494, 0.114494 => 2.482718e-09 \n",
      "-0.000013, -0.000013 => 1.490785e-05 \n",
      "0.015038, 0.015038 => 2.237700e-08 \n",
      "0.030145, 0.030145 => 7.995285e-09 \n",
      "0.063785, 0.063785 => 4.345677e-09 \n",
      "0.037941, 0.037941 => 5.847857e-09 \n",
      "-0.373710, -0.373710 => 8.854189e-10 \n",
      "-0.101241, -0.101241 => 9.868235e-10 \n",
      "Why\n",
      "0.040134, 0.040134 => 1.012221e-08 \n",
      "0.000032, 0.000032 => 1.744729e-05 \n",
      "-0.009732, -0.009732 => 1.913094e-08 \n",
      "1.112160, 1.112160 => 6.402180e-11 \n",
      "-0.272508, -0.272508 => 1.557378e-09 \n",
      "0.010792, 0.010792 => 1.527596e-08 \n",
      "0.002193, 0.002193 => 1.057225e-07 \n",
      "-0.003102, -0.003102 => 1.022073e-07 \n",
      "0.012083, 0.012083 => 5.395590e-09 \n",
      "-0.044034, -0.044034 => 1.753863e-09 \n",
      "bh\n",
      "-0.101406, -0.101406 => 5.191441e-09 \n",
      "-0.577145, -0.577145 => 7.134738e-10 \n",
      "0.008230, 0.008230 => 8.044503e-09 \n",
      "0.001710, 0.001710 => 1.068610e-07 \n",
      "0.146362, 0.146362 => 1.147887e-09 \n",
      "0.000146, 0.000146 => 2.644326e-06 \n",
      "-0.200539, -0.200539 => 1.340820e-09 \n",
      "0.041359, 0.041359 => 6.358851e-09 \n",
      "0.000138, 0.000138 => 3.409754e-07 \n",
      "0.000054, 0.000054 => 5.417110e-06 \n",
      "by\n",
      "0.056440, 0.056440 => 6.845832e-09 \n",
      "-0.372961, -0.372961 => 2.185041e-10 \n",
      "0.306359, 0.306359 => 2.634763e-10 \n",
      "0.001316, 0.001316 => 1.277148e-08 \n",
      "0.004930, 0.004930 => 6.696348e-08 \n",
      "0.002202, 0.002202 => 1.397084e-08 \n",
      "0.002926, 0.002926 => 4.091170e-08 \n",
      "0.509317, 0.509317 => 1.008515e-10 \n",
      "-0.303785, -0.303785 => 6.526889e-10 \n",
      "0.013591, 0.013591 => 1.033943e-08 \n",
      "----\n",
      " c\n",
      "Tepríitopooc\n",
      "Layorelchualpón\n",
      "Savic\n",
      "Tepengao las\n",
      "Rebanta Dlag\n",
      "Tacuól Huecbár Iréna\n",
      "Teruepas\n",
      "Sen\n",
      "Tematíán Orolota Penotemac\n",
      "Tepetla Teacanté\n",
      "Tlapibe dechom\n",
      "Teraz Ríintojitta\n",
      "Loaco\n",
      "Pelca Fería\n",
      "Tupan\n",
      "Pa \n",
      "----\n",
      "iter 5500, loss: 51.205571\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 1.522512e-02 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "0.035881, 0.035881 => 1.089491e-08 \n",
      "-0.000023, -0.000023 => 9.619384e-06 \n",
      "0.025013, 0.025013 => 1.141716e-08 \n",
      "0.015015, 0.015015 => 1.714943e-08 \n",
      "0.513260, 0.513260 => 2.602916e-10 \n",
      "0.000149, 0.000149 => 2.896076e-06 \n",
      "-0.001664, -0.001664 => 2.428296e-08 \n",
      "1.137041, 1.137041 => 4.344804e-10 \n",
      "0.050947, 0.050947 => 8.703422e-09 \n",
      "0.000069, 0.000069 => 7.199936e-06 \n",
      "Why\n",
      "-0.030103, -0.030103 => 3.134114e-09 \n",
      "-1.094175, -1.094175 => 4.503873e-10 \n",
      "0.212309, 0.212309 => 1.852443e-09 \n",
      "-0.061263, -0.061263 => 1.189712e-09 \n",
      "0.003718, 0.003718 => 7.075084e-08 \n",
      "-0.030103, -0.030103 => 6.175322e-09 \n",
      "0.044406, 0.044406 => 6.571008e-09 \n",
      "-0.122002, -0.122002 => 2.387888e-09 \n",
      "0.113246, 0.113246 => 3.866118e-10 \n",
      "0.098411, 0.098411 => 2.096860e-09 \n",
      "bh\n",
      "-0.011264, -0.011264 => 4.147371e-08 \n",
      "-0.000031, -0.000031 => 1.061303e-05 \n",
      "-0.000018, -0.000018 => 6.448805e-06 \n",
      "0.203711, 0.203711 => 8.125152e-10 \n",
      "-0.048301, -0.048301 => 1.333102e-08 \n",
      "0.025061, 0.025061 => 7.057112e-09 \n",
      "-0.001609, -0.001609 => 2.325552e-07 \n",
      "-0.000726, -0.000726 => 1.005916e-07 \n",
      "0.001869, 0.001869 => 7.954489e-08 \n",
      "-0.245704, -0.245704 => 1.687664e-09 \n",
      "by\n",
      "0.063701, 0.063701 => 1.291866e-09 \n",
      "0.249050, 0.249050 => 2.496588e-09 \n",
      "0.136615, 0.136615 => 2.134091e-09 \n",
      "-0.884878, -0.884878 => 3.586585e-10 \n",
      "0.335016, 0.335016 => 2.574737e-10 \n",
      "0.093639, 0.093639 => 3.164798e-09 \n",
      "0.564217, 0.564217 => 8.148635e-10 \n",
      "0.139966, 0.139966 => 1.965323e-09 \n",
      "0.001766, 0.001766 => 3.612714e-08 \n",
      "0.585750, 0.585750 => 6.324433e-12 \n",
      "----\n",
      " \n",
      "CóAtixoe tumcucoquerio Lhubotimo\n",
      "Zatariz\n",
      "El Líparo de Tlán Arongo Flotza\n",
      "Cóntaht\n",
      "Guitepen\n",
      "Angaldenatechi\n",
      "ZarláOna\n",
      "Ahidhen\n",
      "Esiquíelas\n",
      "El Mopogo\n",
      "Haban\n",
      "Ruen Freníí\n",
      "Dojoto dodec\n",
      "Coyacaro\n",
      "Ala\n",
      "Cadaro\n",
      "Cucal \n",
      "----\n",
      "iter 6000, loss: 52.848682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GradCheck\n",
      "Wxh\n",
      "-0.000037, -0.000037 => 1.072437e-05 \n",
      "-0.000032, -0.000032 => 3.011591e-06 \n",
      "-0.000000, -0.000000 => 2.822715e-03 \n",
      "0.000008, 0.000008 => 9.048807e-06 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 2.531521e-02 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "-0.006482, -0.006482 => 7.813510e-08 \n",
      "0.000274, 0.000274 => 4.697178e-07 \n",
      "-0.602677, -0.602677 => 4.124782e-10 \n",
      "0.270507, 0.270507 => 1.808450e-09 \n",
      "-0.455913, -0.455913 => 3.631229e-10 \n",
      "-0.391301, -0.391301 => 1.392445e-10 \n",
      "0.437854, 0.437854 => 8.043587e-10 \n",
      "-0.004479, -0.004479 => 4.053980e-08 \n",
      "0.000548, 0.000548 => 4.081023e-08 \n",
      "0.150917, 0.150917 => 4.074459e-10 \n",
      "Why\n",
      "0.002979, 0.002979 => 4.781528e-08 \n",
      "-0.946687, -0.946687 => 8.883007e-11 \n",
      "-0.009894, -0.009894 => 2.126482e-09 \n",
      "-0.011222, -0.011222 => 3.182037e-08 \n",
      "-0.894319, -0.894319 => 2.468055e-10 \n",
      "0.022449, 0.022449 => 4.840311e-09 \n",
      "-0.004896, -0.004896 => 4.051236e-08 \n",
      "-0.000306, -0.000306 => 7.120199e-07 \n",
      "-0.842142, -0.842142 => 2.031686e-10 \n",
      "-0.006830, -0.006830 => 2.289040e-08 \n",
      "bh\n",
      "0.444976, 0.444976 => 2.730915e-10 \n",
      "-0.013021, -0.013021 => 5.813672e-09 \n",
      "0.000006, 0.000006 => 8.661098e-06 \n",
      "0.022724, 0.022724 => 8.274382e-09 \n",
      "-0.053359, -0.053359 => 3.126364e-09 \n",
      "0.342938, 0.342938 => 9.752556e-10 \n",
      "0.228275, 0.228275 => 1.150152e-09 \n",
      "0.000206, 0.000206 => 1.543172e-06 \n",
      "-0.147470, -0.147470 => 6.840328e-10 \n",
      "0.000018, 0.000018 => 5.355768e-06 \n",
      "by\n",
      "0.001203, 0.001203 => 1.656149e-07 \n",
      "0.638212, 0.638212 => 5.607748e-10 \n",
      "0.088120, 0.088120 => 1.891065e-10 \n",
      "-0.123815, -0.123815 => 1.397641e-09 \n",
      "0.042728, 0.042728 => 4.725994e-09 \n",
      "0.001247, 0.001247 => 1.105447e-07 \n",
      "0.026806, 0.026806 => 2.270663e-09 \n",
      "-0.945059, -0.945059 => 3.476862e-10 \n",
      "0.176300, 0.176300 => 5.039028e-10 \n",
      "0.038773, 0.038773 => 3.063646e-09 \n",
      "----\n",
      " n Latiavin\n",
      "San Juana Juáñac\n",
      "San Juan\n",
      "San Covaatorlo\n",
      "San Juón Pociro\n",
      "San Leonteza Imen Acimaróc Pezga Znonaó\n",
      "IbYuc\n",
      "San Oman\n",
      "San Juiano Pón Méruay\n",
      "Juandrapan del\n",
      "Sen Jutemotia\n",
      "San Tubtlán Gachuito\n",
      "San M \n",
      "----\n",
      "iter 6500, loss: 52.882883\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 5.558881e-04 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 3.127439e-03 \n",
      "Whh\n",
      "-0.462149, -0.462149 => 8.941157e-10 \n",
      "0.005021, 0.005021 => 4.175105e-08 \n",
      "0.000879, 0.000879 => 2.596059e-07 \n",
      "-0.328785, -0.328785 => 2.274398e-09 \n",
      "-1.200698, -1.200698 => 2.215378e-10 \n",
      "-0.278473, -0.278473 => 8.740434e-11 \n",
      "-0.003341, -0.003341 => 2.946991e-08 \n",
      "0.015562, 0.015562 => 1.856098e-08 \n",
      "0.016561, 0.016561 => 7.460962e-10 \n",
      "0.000237, 0.000237 => 2.221836e-06 \n",
      "Why\n",
      "0.051894, 0.051894 => 7.930127e-10 \n",
      "-0.061694, -0.061694 => 1.642933e-09 \n",
      "-0.001234, -0.001234 => 6.722506e-08 \n",
      "-1.039455, -1.039455 => 7.661630e-11 \n",
      "-0.051822, -0.051822 => 5.679017e-12 \n",
      "-1.535093, -1.535093 => 1.662853e-10 \n",
      "-0.005694, -0.005694 => 3.964090e-08 \n",
      "-0.001105, -0.001105 => 1.140604e-07 \n",
      "0.041090, 0.041090 => 4.343711e-09 \n",
      "0.017984, 0.017984 => 1.213147e-08 \n",
      "bh\n",
      "-0.000119, -0.000119 => 3.390638e-06 \n",
      "-0.004481, -0.004481 => 5.129690e-09 \n",
      "-0.001289, -0.001289 => 3.182208e-07 \n",
      "-0.000164, -0.000164 => 1.668960e-06 \n",
      "-1.104923, -1.104923 => 1.889034e-10 \n",
      "-0.004481, -0.004481 => 5.129690e-09 \n",
      "-0.000049, -0.000049 => 4.413264e-08 \n",
      "1.009771, 1.009771 => 6.873007e-10 \n",
      "0.468884, 0.468884 => 4.249007e-10 \n",
      "-0.000513, -0.000513 => 2.463475e-07 \n",
      "by\n",
      "0.202761, 0.202761 => 4.950141e-10 \n",
      "0.007148, 0.007148 => 1.671715e-08 \n",
      "0.059832, 0.059832 => 2.407790e-09 \n",
      "0.010098, 0.010098 => 2.361890e-08 \n",
      "0.022697, 0.022697 => 6.584407e-09 \n",
      "0.202761, 0.202761 => 4.950141e-10 \n",
      "0.087787, 0.087787 => 1.235230e-09 \n",
      "0.051445, 0.051445 => 2.147223e-09 \n",
      "0.524332, 0.524332 => 3.849959e-10 \n",
      "-0.853450, -0.853450 => 1.763836e-11 \n",
      "----\n",
      " a\n",
      "Maycutc desuidíe Teteperen\n",
      "Torco\n",
      "Tepel Sen dillo\n",
      "Tuquécihuacoz\n",
      "Tuvicango de PPejetzacatíco\n",
      "Tuxtlán\n",
      "Tenilad Teelo\n",
      "Taokánjentiz\n",
      "To\n",
      "Teríe Cuco TCanghontl Taninen\n",
      "Tacemián Tláruntilla\n",
      "Teltla Tiecuán\n",
      "Cuz \n",
      "----\n",
      "iter 7000, loss: 50.172120\n",
      "---GradCheck\n",
      "Wxh\n",
      "0.189769, 0.189769 => 4.438077e-10 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "-0.000345, -0.000345 => 2.341755e-07 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "0.000000, 0.000000 => 0.000000e+00 \n",
      "Whh\n",
      "-0.000370, -0.000370 => 3.302880e-07 \n",
      "0.000400, 0.000400 => 7.810783e-07 \n",
      "0.000690, 0.000690 => 6.267631e-07 \n",
      "0.000539, 0.000539 => 4.988887e-07 \n",
      "0.013111, 0.013111 => 2.286902e-08 \n",
      "0.738428, 0.738428 => 6.088563e-11 \n",
      "-0.589780, -0.589780 => 8.387731e-10 \n",
      "-0.001231, -0.001231 => 2.641910e-08 \n",
      "0.683955, 0.683955 => 2.681305e-10 \n",
      "0.484994, 0.484994 => 2.003825e-10 \n",
      "Why\n",
      "0.068202, 0.068202 => 3.823294e-09 \n",
      "-0.046105, -0.046105 => 3.944247e-09 \n",
      "0.041871, 0.041871 => 3.772282e-09 \n",
      "2.036022, 2.036022 => 1.504494e-10 \n",
      "-0.026569, -0.026569 => 3.778031e-09 \n",
      "-0.003924, -0.003924 => 2.222604e-08 \n",
      "0.046354, 0.046354 => 2.909914e-12 \n",
      "-0.408789, -0.408789 => 4.407566e-10 \n",
      "-0.001014, -0.001014 => 1.867978e-07 \n",
      "0.311066, 0.311066 => 2.044676e-10 \n",
      "bh\n",
      "-0.000037, -0.000037 => 4.324657e-06 \n",
      "0.120410, 0.120410 => 1.154614e-09 \n",
      "-0.064071, -0.064071 => 2.089330e-09 \n",
      "0.088051, 0.088051 => 4.215245e-10 \n",
      "1.566965, 1.566965 => 2.333150e-11 \n",
      "1.163786, 1.163786 => 2.686423e-11 \n",
      "0.072405, 0.072405 => 2.693081e-09 \n",
      "0.199842, 0.199842 => 7.945363e-10 \n",
      "-0.003621, -0.003621 => 1.313916e-07 \n",
      "0.445146, 0.445146 => 3.470325e-10 \n",
      "by\n",
      "0.117942, 0.117942 => 4.783534e-10 \n",
      "0.448377, 0.448377 => 2.640485e-10 \n",
      "0.000780, 0.000780 => 6.427652e-08 \n",
      "2.315273, 2.315273 => 2.725993e-11 \n",
      "0.005356, 0.005356 => 1.823681e-08 \n",
      "0.009316, 0.009316 => 1.263770e-08 \n",
      "0.071040, 0.071040 => 3.246155e-09 \n",
      "0.127642, 0.127642 => 1.461196e-09 \n",
      "-0.580153, -0.580153 => 8.224683e-12 \n",
      "0.059328, 0.059328 => 4.457409e-09 \n",
      "----\n",
      " uza\n",
      "Ixtalco\n",
      "Guajotalla Ipaz\n",
      "Fattiméma\n",
      "Oz Frolta\n",
      "Gucedez Molapa\n",
      "Juxhtete delio\n",
      "Chupepucon\n",
      "Ez Penco\n",
      "Huinaro\n",
      "Yeyipo\n",
      "Quiacho\n",
      "ViDíid\n",
      "Hueo Cuaran\n",
      "Tepanco Cehuagero\n",
      "Igomamti\n",
      "Huapot Cecitopa\n",
      "Ahuarecam\n",
      "Xilo In \n",
      "----\n",
      "iter 7500, loss: 52.247587\n"
     ]
    }
   ],
   "source": [
    "num_iter = 7501\n",
    "while n<num_iter:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    \n",
    "    # sample from the model now and then\n",
    "    if n % 500 == 0:\n",
    "        # gradient checking\n",
    "        print(\"---GradCheck\")\n",
    "        gradCheck(inputs, targets, hprev)\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 500 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes recurrentes tipo LSTM\n",
    "\n",
    "Las redes con unidades LSTM las vimos platicadas en clase, pero no hay nada mejor para entender un tema que implementarlo y comparar los resultados con la red recurrente simple, sin memoria, para esto vamos a hacer lo mismo que antes, pero con unidades LSTM.\n",
    "\n",
    "Para esto vamos a utilizar otro [gist](https://gist.github.com/karpathy/587454dc0146a6ae21fc) del mismo autor (que es una referencia obligada en el tema, por cierto). En este *gist*, el autor presenta un modelo de redes recurrentes LSTM desarrollado con *numpy* incluido el método de entrenamiento, pero no lo aplica a el modelado *letra a letra* como el gist pasado. Para esta parte de la libreta, lo que tienen que realizar es lo siguiente:\n",
    "\n",
    "\n",
    "1. Copiiar el contenido del *gist* y comentarlo en español (y cambiar algo de código de forma que quede mñas claro para ti y para mi).\n",
    "\n",
    "2. Adaptar el modelo propuesto para usarlo en la generación de nombres de municipios.\n",
    "\n",
    "3. Ajustar los hiperparámetros del modelo, así como los parámetros del algoritmo de entrenamiento con el fin de generar una lista de nombres de municipios creibles, pero sin sobreaprendizaje (esto es, que copie vilmente el nombre de municipios existentes). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    @staticmethod\n",
    "    def init(input_size, hidden_size, fancy_forget_bias_init = 3):\n",
    "        \"\"\" \n",
    "        Initialize parameters of the LSTM (both weights and biases in one matrix) \n",
    "        One might way to have a positive fancy_forget_bias_init number (e.g. maybe even up to 5, in some papers)\n",
    "        \"\"\"\n",
    "        # +1 for the biases, which will be the first row of WLSTM\n",
    "        WLSTM = np.random.randn(input_size + hidden_size + 1, 4 * hidden_size) / np.sqrt(input_size + hidden_size)\n",
    "        WLSTM[0,:] = 0 # initialize biases to zero\n",
    "        if fancy_forget_bias_init != 0:\n",
    "            # forget gates get little bit negative bias initially to encourage them to be turned off\n",
    "            # remember that due to Xavier initialization above, the raw output activations from gates before\n",
    "            # nonlinearity are zero mean and on order of standard deviation ~1\n",
    "            WLSTM[0,hidden_size:2*hidden_size] = fancy_forget_bias_init\n",
    "        return WLSTM\n",
    "  \n",
    "    @staticmethod\n",
    "    def forward(X, WLSTM, c0 = None, h0 = None):\n",
    "        \"\"\"\n",
    "        X should be of shape (n,b,input_size), where n = length of sequence, b = batch size\n",
    "        \"\"\"\n",
    "        n,b,input_size = X.shape\n",
    "        \n",
    "        d = int(WLSTM.shape[1]/4) # hidden size\n",
    "        if c0 is None: \n",
    "            c0 = np.zeros((b,d))\n",
    "        if h0 is None: \n",
    "            h0 = np.zeros((b,d))\n",
    "\n",
    "        # Perform the LSTM forward pass with X as the input\n",
    "        xphpb = WLSTM.shape[0] # x plus h plus bias, lol\n",
    "        Hin = np.zeros((n, b, xphpb)) # input [1, xt, ht-1] to each tick of the LSTM\n",
    "        Hout = np.zeros((n, b, d)) # hidden representation of the LSTM (gated cell content)\n",
    "        IFOG = np.zeros((n, b, d * 4)) # input, forget, output, gate (IFOG)\n",
    "        IFOGf = np.zeros((n, b, d * 4)) # after nonlinearity\n",
    "        C = np.zeros((n, b, d)) # cell content\n",
    "        Ct = np.zeros((n, b, d)) # tanh of cell content\n",
    "        for t in range(n):\n",
    "            # concat [x,h] as input to the LSTM\n",
    "            prevh = Hout[t-1] if t > 0 else h0\n",
    "            Hin[t,:,0] = 1 # bias\n",
    "            Hin[t,:,1:input_size+1] = X[t]\n",
    "            Hin[t,:,input_size+1:] = prevh\n",
    "            # compute all gate activations. dots: (most work is this line)\n",
    "            IFOG[t] = Hin[t].dot(WLSTM)\n",
    "            # non-linearities\n",
    "            IFOGf[t,:,:int(3*d)] = 1.0/(1.0+np.exp(-IFOG[t,:,:int(3*d)])) # sigmoids; these are the gates\n",
    "            IFOGf[t,:,3*d:] = np.tanh(IFOG[t,:,3*d:]) # tanh\n",
    "            # compute the cell activation\n",
    "            prevc = C[t-1] if t > 0 else c0\n",
    "            C[t] = IFOGf[t,:,:d] * IFOGf[t,:,3*d:] + IFOGf[t,:,d:2*d] * prevc\n",
    "            Ct[t] = np.tanh(C[t])\n",
    "            Hout[t] = IFOGf[t,:,2*d:3*d] * Ct[t]\n",
    "\n",
    "        cache = {}\n",
    "        cache['WLSTM'] = WLSTM\n",
    "        cache['Hout'] = Hout\n",
    "        cache['IFOGf'] = IFOGf\n",
    "        cache['IFOG'] = IFOG\n",
    "        cache['C'] = C\n",
    "        cache['Ct'] = Ct\n",
    "        cache['Hin'] = Hin\n",
    "        cache['c0'] = c0\n",
    "        cache['h0'] = h0\n",
    "\n",
    "        # return C[t], as well so we can continue LSTM with prev state init if needed\n",
    "        return Hout, C[t], Hout[t], cache\n",
    "  \n",
    "    @staticmethod\n",
    "    def backward(dHout_in, cache, dcn = None, dhn = None): \n",
    "\n",
    "        WLSTM = cache['WLSTM']\n",
    "        Hout = cache['Hout']\n",
    "        IFOGf = cache['IFOGf']\n",
    "        IFOG = cache['IFOG']\n",
    "        C = cache['C']\n",
    "        Ct = cache['Ct']\n",
    "        Hin = cache['Hin']\n",
    "        c0 = cache['c0']\n",
    "        h0 = cache['h0']\n",
    "        n,b,d = Hout.shape\n",
    "        input_size = WLSTM.shape[0] - d - 1 # -1 due to bias\n",
    "\n",
    "        # backprop the LSTM\n",
    "        dIFOG = np.zeros(IFOG.shape)\n",
    "        dIFOGf = np.zeros(IFOGf.shape)\n",
    "        dWLSTM = np.zeros(WLSTM.shape)\n",
    "        dHin = np.zeros(Hin.shape)\n",
    "        dC = np.zeros(C.shape)\n",
    "        dX = np.zeros((n,b,input_size))\n",
    "        dh0 = np.zeros((b, d))\n",
    "        dc0 = np.zeros((b, d))\n",
    "        dHout = dHout_in.copy() # make a copy so we don't have any funny side effects\n",
    "        \n",
    "        if dcn is not None: \n",
    "            dC[n-1] += dcn.copy() # carry over gradients from later\n",
    "        if dhn is not None: \n",
    "            dHout[n-1] += dhn.copy()\n",
    "        \n",
    "        for t in reversed(range(n)):\n",
    "\n",
    "            tanhCt = Ct[t]\n",
    "            dIFOGf[t,:,2*d:3*d] = tanhCt * dHout[t]\n",
    "            # backprop tanh non-linearity first then continue backprop\n",
    "            dC[t] += (1-tanhCt**2) * (IFOGf[t,:,2*d:3*d] * dHout[t])\n",
    "\n",
    "            if t > 0:\n",
    "                dIFOGf[t,:,d:2*d] = C[t-1] * dC[t]\n",
    "                dC[t-1] += IFOGf[t,:,d:2*d] * dC[t]\n",
    "            else:\n",
    "                dIFOGf[t,:,d:2*d] = c0 * dC[t]\n",
    "                dc0 = IFOGf[t,:,d:2*d] * dC[t]\n",
    "            dIFOGf[t,:,:d] = IFOGf[t,:,3*d:] * dC[t]\n",
    "            dIFOGf[t,:,3*d:] = IFOGf[t,:,:d] * dC[t]\n",
    "\n",
    "            # backprop activation functions\n",
    "            dIFOG[t,:,3*d:] = (1 - IFOGf[t,:,3*d:] ** 2) * dIFOGf[t,:,3*d:]\n",
    "            y = IFOGf[t,:,:3*d]\n",
    "            dIFOG[t,:,:3*d] = (y*(1.0-y)) * dIFOGf[t,:,:3*d]\n",
    "\n",
    "            # backprop matrix multiply\n",
    "            dWLSTM += np.dot(Hin[t].transpose(), dIFOG[t])\n",
    "            dHin[t] = dIFOG[t].dot(WLSTM.transpose())\n",
    "\n",
    "            # backprop the identity transforms into Hin\n",
    "            dX[t] = dHin[t,:,1:input_size+1]\n",
    "            if t > 0:\n",
    "                dHout[t-1,:] += dHin[t,:,input_size+1:]\n",
    "            else:\n",
    "                dh0 += dHin[t,:,input_size+1:]\n",
    "\n",
    "        return dX, dWLSTM, dc0, dh0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# TEST CASES\n",
    "# -------------------\n",
    "\n",
    "def checkSequentialMatchesBatch():\n",
    "    \"\"\" check LSTM I/O forward/backward interactions \"\"\"\n",
    "    n = 5 # sequence length\n",
    "    b = 3 # batch size\n",
    "    d = 4 # hidden size\n",
    "    input_size = 10\n",
    "    WLSTM = LSTM.init(input_size, d) # input size, hidden size\n",
    "    X = np.random.randn(n,b,input_size)\n",
    "    h0 = np.random.randn(b,d)\n",
    "    c0 = np.random.randn(b,d)\n",
    "\n",
    "    # sequential forward\n",
    "    cprev = c0\n",
    "    hprev = h0\n",
    "    caches = [{} for t in range(n)]\n",
    "    Hcat = np.zeros((n,b,d))\n",
    "    for t in range(n):\n",
    "        xt = X[t:t+1]\n",
    "        _, cprev, hprev, cache = LSTM.forward(xt, WLSTM, cprev, hprev)\n",
    "        caches[t] = cache\n",
    "        Hcat[t] = hprev\n",
    "\n",
    "    # sanity check: perform batch forward to check that we get the same thing\n",
    "    H, _, _, batch_cache = LSTM.forward(X, WLSTM, c0, h0)\n",
    "    assert np.allclose(H, Hcat), 'Sequential and Batch forward don''t match!'\n",
    "\n",
    "    # eval loss\n",
    "    wrand = np.random.randn(*Hcat.shape)\n",
    "    loss = np.sum(Hcat * wrand)\n",
    "    dH = wrand\n",
    "\n",
    "    # get the batched version gradients\n",
    "    BdX, BdWLSTM, Bdc0, Bdh0 = LSTM.backward(dH, batch_cache)\n",
    "\n",
    "    # now perform sequential backward\n",
    "    dX = np.zeros_like(X)\n",
    "    dWLSTM = np.zeros_like(WLSTM)\n",
    "    dc0 = np.zeros_like(c0)\n",
    "    dh0 = np.zeros_like(h0)\n",
    "    dcnext = None\n",
    "    dhnext = None\n",
    "    for t in reversed(range(n)):\n",
    "        dht = dH[t].reshape(1, b, d)\n",
    "        dx, dWLSTMt, dcprev, dhprev = LSTM.backward(dht, caches[t], dcnext, dhnext)\n",
    "        dhnext = dhprev\n",
    "        dcnext = dcprev\n",
    "\n",
    "        dWLSTM += dWLSTMt # accumulate LSTM gradient\n",
    "        dX[t] = dx[0]\n",
    "        if t == 0:\n",
    "            dc0 = dcprev\n",
    "            dh0 = dhprev\n",
    "\n",
    "    # and make sure the gradients match\n",
    "    print ('Making sure batched version agrees with sequential version: (should all be True)')\n",
    "    print (np.allclose(BdX, dX))\n",
    "    print (np.allclose(BdWLSTM, dWLSTM))\n",
    "    print (np.allclose(Bdc0, dc0))\n",
    "    print (np.allclose(Bdh0, dh0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBatchGradient():\n",
    "    \"\"\" check that the batch gradient is correct \"\"\"\n",
    "\n",
    "    # lets gradient check this beast\n",
    "    n,b,d = (5, 3, 4) # sequence length, batch size, hidden size\n",
    "    input_size = 10\n",
    "    WLSTM = LSTM.init(input_size, d) # input size, hidden size\n",
    "    X = np.random.randn(n,b,input_size)\n",
    "    h0 = np.random.randn(b,d)\n",
    "    c0 = np.random.randn(b,d)\n",
    "\n",
    "    # batch forward backward\n",
    "    H, Ct, Ht, cache = LSTM.forward(X, WLSTM, c0, h0)\n",
    "    wrand = np.random.randn(*H.shape)\n",
    "    loss = np.sum(H * wrand) # weighted sum is a nice hash to use I think\n",
    "    dH = wrand\n",
    "    dX, dWLSTM, dc0, dh0 = LSTM.backward(dH, cache)\n",
    "\n",
    "    def fwd():\n",
    "        h,_,_,_ = LSTM.forward(X, WLSTM, c0, h0)\n",
    "        return np.sum(h * wrand)\n",
    "\n",
    "    # now gradient check all\n",
    "    delta = 1e-5\n",
    "    rel_error_thr_warning = 1e-2\n",
    "    rel_error_thr_error = 1\n",
    "    tocheck = [X, WLSTM, c0, h0]\n",
    "    grads_analytic = [dX, dWLSTM, dc0, dh0]\n",
    "    names = ['X', 'WLSTM', 'c0', 'h0']\n",
    "    for j in range(len(tocheck)):\n",
    "        mat = tocheck[j]\n",
    "        dmat = grads_analytic[j]\n",
    "        name = names[j]\n",
    "        # gradcheck\n",
    "        for i in range(mat.size):\n",
    "            old_val = mat.flat[i]\n",
    "            mat.flat[i] = old_val + delta\n",
    "            loss0 = fwd()\n",
    "            mat.flat[i] = old_val - delta\n",
    "            loss1 = fwd()\n",
    "            mat.flat[i] = old_val\n",
    "\n",
    "            grad_analytic = dmat.flat[i]\n",
    "            grad_numerical = (loss0 - loss1) / (2 * delta)\n",
    "\n",
    "            if grad_numerical == 0 and grad_analytic == 0:\n",
    "                rel_error = 0 # both are zero, OK.\n",
    "                status = 'OK'\n",
    "            elif abs(grad_numerical) < 1e-7 and abs(grad_analytic) < 1e-7:\n",
    "                rel_error = 0 # not enough precision to check this\n",
    "                status = 'VAL SMALL WARNING'\n",
    "            else:\n",
    "                rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "                status = 'OK'\n",
    "            if rel_error > rel_error_thr_warning:\n",
    "                status = 'WARNING'\n",
    "            if rel_error > rel_error_thr_error:\n",
    "                status = '!!!!! NOTOK'\n",
    "\n",
    "            # print stats\n",
    "            print ('%s checking param %s index %s (val = %+8f), analytic = %+8f, numerical = %+8f, relative error = %+8f' \n",
    "                % (status, name, np.unravel_index(i, mat.shape), old_val, grad_analytic, grad_numerical, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making sure batched version agrees with sequential version: (should all be True)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "should all be True\n",
      "OK checking param X index (0, 0, 0) (val = +0.179870), analytic = +0.219229, numerical = +0.219229, relative error = +0.000000\n",
      "OK checking param X index (0, 0, 1) (val = +0.686874), analytic = +0.241156, numerical = +0.241156, relative error = +0.000000\n",
      "OK checking param X index (0, 0, 2) (val = -0.773984), analytic = -0.053245, numerical = -0.053245, relative error = +0.000000\n",
      "OK checking param X index (0, 0, 3) (val = +1.405523), analytic = -0.220690, numerical = -0.220690, relative error = +0.000000\n",
      "OK checking param X index (0, 0, 4) (val = +0.177581), analytic = -0.038497, numerical = -0.038497, relative error = +0.000000\n",
      "OK checking param X index (0, 0, 5) (val = +1.108462), analytic = -0.092004, numerical = -0.092004, relative error = +0.000000\n",
      "OK checking param X index (0, 0, 6) (val = -1.461207), analytic = -0.252768, numerical = -0.252768, relative error = +0.000000\n",
      "OK checking param X index (0, 0, 7) (val = -0.442391), analytic = +0.017981, numerical = +0.017981, relative error = +0.000000\n",
      "OK checking param X index (0, 0, 8) (val = -0.637348), analytic = +0.137365, numerical = +0.137365, relative error = +0.000000\n",
      "OK checking param X index (0, 0, 9) (val = +0.291192), analytic = +0.025963, numerical = +0.025963, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 0) (val = +1.855941), analytic = +0.103872, numerical = +0.103872, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 1) (val = -0.109782), analytic = -0.098278, numerical = -0.098278, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 2) (val = -0.089760), analytic = -0.104236, numerical = -0.104236, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 3) (val = +0.325778), analytic = -0.100574, numerical = -0.100574, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 4) (val = +0.802182), analytic = +0.016183, numerical = +0.016183, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 5) (val = +0.984474), analytic = -0.076835, numerical = -0.076835, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 6) (val = -0.148849), analytic = -0.060655, numerical = -0.060655, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 7) (val = -0.247468), analytic = +0.186312, numerical = +0.186312, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 8) (val = +0.323170), analytic = -0.226312, numerical = -0.226312, relative error = +0.000000\n",
      "OK checking param X index (0, 1, 9) (val = +1.836828), analytic = -0.192193, numerical = -0.192193, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 0) (val = -0.955743), analytic = +0.043207, numerical = +0.043207, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 1) (val = +1.075425), analytic = -0.006999, numerical = -0.006999, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 2) (val = +0.171531), analytic = +0.048280, numerical = +0.048280, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 3) (val = +0.233974), analytic = -0.125782, numerical = -0.125782, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 4) (val = -0.799106), analytic = +0.183042, numerical = +0.183042, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 5) (val = -1.999963), analytic = -0.033175, numerical = -0.033175, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 6) (val = -0.145100), analytic = +0.166905, numerical = +0.166905, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 7) (val = -1.845523), analytic = +0.066767, numerical = +0.066767, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 8) (val = +0.126164), analytic = -0.245654, numerical = -0.245654, relative error = +0.000000\n",
      "OK checking param X index (0, 2, 9) (val = -0.315639), analytic = +0.099512, numerical = +0.099512, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 0) (val = +0.245195), analytic = +0.279991, numerical = +0.279991, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 1) (val = +0.723358), analytic = +0.132761, numerical = +0.132761, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 2) (val = -1.212592), analytic = +0.063296, numerical = +0.063296, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 3) (val = -0.081225), analytic = -0.147529, numerical = -0.147529, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 4) (val = -1.064582), analytic = -0.035302, numerical = -0.035302, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 5) (val = +1.225361), analytic = +0.023722, numerical = +0.023722, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 6) (val = +0.232521), analytic = +0.048021, numerical = +0.048021, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 7) (val = -0.025840), analytic = +0.030792, numerical = +0.030792, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 8) (val = +0.495586), analytic = -0.148435, numerical = -0.148435, relative error = +0.000000\n",
      "OK checking param X index (1, 0, 9) (val = -1.440755), analytic = +0.063070, numerical = +0.063070, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 0) (val = -0.136063), analytic = +0.200704, numerical = +0.200704, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 1) (val = -0.230157), analytic = -0.030110, numerical = -0.030110, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 2) (val = +0.047844), analytic = -0.235322, numerical = -0.235322, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 3) (val = -0.901689), analytic = -0.113509, numerical = -0.113509, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 4) (val = +0.097478), analytic = +0.100496, numerical = +0.100496, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 5) (val = -0.173553), analytic = -0.054740, numerical = -0.054740, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 6) (val = -0.304398), analytic = -0.075907, numerical = -0.075907, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 7) (val = +0.609500), analytic = +0.073932, numerical = +0.073932, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 8) (val = +0.927848), analytic = -0.536670, numerical = -0.536670, relative error = +0.000000\n",
      "OK checking param X index (1, 1, 9) (val = +0.189234), analytic = -0.124083, numerical = -0.124083, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 0) (val = -0.099533), analytic = -0.075247, numerical = -0.075247, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 1) (val = -0.420615), analytic = +0.001184, numerical = +0.001184, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 2) (val = +0.732991), analytic = +0.023844, numerical = +0.023844, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 3) (val = +0.094993), analytic = -0.005075, numerical = -0.005075, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 4) (val = -0.422504), analytic = +0.033780, numerical = +0.033780, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 5) (val = +0.145853), analytic = +0.042839, numerical = +0.042839, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 6) (val = -0.625531), analytic = +0.030317, numerical = +0.030317, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 7) (val = -0.649832), analytic = -0.003266, numerical = -0.003266, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 8) (val = -0.272555), analytic = +0.012365, numerical = +0.012365, relative error = +0.000000\n",
      "OK checking param X index (1, 2, 9) (val = +0.468424), analytic = -0.008144, numerical = -0.008144, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 0) (val = -0.952415), analytic = -0.039293, numerical = -0.039293, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 1) (val = -0.493713), analytic = +0.064903, numerical = +0.064903, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 2) (val = +0.281244), analytic = -0.042074, numerical = -0.042074, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 3) (val = -1.169416), analytic = -0.031754, numerical = -0.031754, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 4) (val = -0.116793), analytic = +0.068770, numerical = +0.068770, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 5) (val = -0.812402), analytic = -0.071494, numerical = -0.071494, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 6) (val = -0.106459), analytic = -0.145173, numerical = -0.145173, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 7) (val = +0.250832), analytic = -0.067143, numerical = -0.067143, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 8) (val = -1.822261), analytic = +0.112457, numerical = +0.112457, relative error = +0.000000\n",
      "OK checking param X index (2, 0, 9) (val = -0.851705), analytic = +0.001419, numerical = +0.001419, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 0) (val = +1.102948), analytic = +0.188907, numerical = +0.188907, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 1) (val = +0.011468), analytic = +0.005982, numerical = +0.005982, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 2) (val = -0.754067), analytic = -0.255821, numerical = -0.255821, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 3) (val = -0.102041), analytic = -0.323250, numerical = -0.323250, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 4) (val = +0.220896), analytic = +0.100143, numerical = +0.100143, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 5) (val = +1.367426), analytic = -0.243157, numerical = -0.243157, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 6) (val = +0.074515), analytic = +0.006884, numerical = +0.006884, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 7) (val = +0.299992), analytic = +0.262858, numerical = +0.262858, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 8) (val = -0.055350), analytic = -0.386988, numerical = -0.386988, relative error = +0.000000\n",
      "OK checking param X index (2, 1, 9) (val = -1.021576), analytic = +0.102428, numerical = +0.102428, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 0) (val = -1.963999), analytic = -0.109135, numerical = -0.109135, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 1) (val = -0.546586), analytic = -0.019015, numerical = -0.019015, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 2) (val = +0.113187), analytic = -0.135617, numerical = -0.135617, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 3) (val = +0.491237), analytic = -0.009332, numerical = -0.009332, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 4) (val = -1.746086), analytic = +0.054520, numerical = +0.054520, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 5) (val = -0.377077), analytic = -0.037643, numerical = -0.037643, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 6) (val = -0.127630), analytic = -0.069701, numerical = -0.069701, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 7) (val = -0.841290), analytic = -0.102885, numerical = -0.102885, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 8) (val = -0.197781), analytic = +0.118100, numerical = +0.118100, relative error = +0.000000\n",
      "OK checking param X index (2, 2, 9) (val = +0.181723), analytic = -0.084011, numerical = -0.084011, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 0) (val = +0.410828), analytic = +0.088498, numerical = +0.088498, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 1) (val = -0.443473), analytic = -0.067616, numerical = -0.067616, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 2) (val = +0.501701), analytic = +0.077610, numerical = +0.077610, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 3) (val = +0.117424), analytic = +0.003084, numerical = +0.003084, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 4) (val = +0.799846), analytic = -0.001017, numerical = -0.001017, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 5) (val = +0.543254), analytic = +0.086048, numerical = +0.086048, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 6) (val = -0.305109), analytic = +0.222683, numerical = +0.222683, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 7) (val = -0.857739), analytic = +0.027525, numerical = +0.027525, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 8) (val = +0.670128), analytic = -0.231105, numerical = -0.231105, relative error = +0.000000\n",
      "OK checking param X index (3, 0, 9) (val = +1.065636), analytic = +0.066794, numerical = +0.066794, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 0) (val = -1.999767), analytic = +0.227154, numerical = +0.227154, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 1) (val = -0.566659), analytic = -0.081807, numerical = -0.081807, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 2) (val = +0.237284), analytic = -0.021070, numerical = -0.021070, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 3) (val = -1.044427), analytic = -0.156413, numerical = -0.156413, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 4) (val = +0.374132), analytic = +0.173815, numerical = +0.173815, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 5) (val = +1.066729), analytic = -0.126032, numerical = -0.126032, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 6) (val = +0.165810), analytic = +0.237046, numerical = +0.237046, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 7) (val = -1.413635), analytic = +0.054510, numerical = +0.054510, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 8) (val = +0.533903), analytic = -0.460938, numerical = -0.460938, relative error = +0.000000\n",
      "OK checking param X index (3, 1, 9) (val = +1.476752), analytic = +0.134656, numerical = +0.134656, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 0) (val = +0.184540), analytic = -0.219006, numerical = -0.219006, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 1) (val = -0.997807), analytic = +0.056292, numerical = +0.056292, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 2) (val = -1.024773), analytic = -0.156039, numerical = -0.156039, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 3) (val = +0.212336), analytic = -0.098029, numerical = -0.098029, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 4) (val = -0.490255), analytic = +0.077586, numerical = +0.077586, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 5) (val = -0.794156), analytic = -0.244034, numerical = -0.244034, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 6) (val = +0.143886), analytic = -0.341597, numerical = -0.341597, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 7) (val = +0.437000), analytic = +0.003677, numerical = +0.003677, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 8) (val = +0.077120), analytic = +0.331138, numerical = +0.331138, relative error = +0.000000\n",
      "OK checking param X index (3, 2, 9) (val = -1.031438), analytic = -0.059324, numerical = -0.059324, relative error = +0.000000\n",
      "OK checking param X index (4, 0, 0) (val = -1.577156), analytic = -0.122182, numerical = -0.122182, relative error = +0.000000\n",
      "OK checking param X index (4, 0, 1) (val = +0.112444), analytic = +0.066347, numerical = +0.066347, relative error = +0.000000\n",
      "OK checking param X index (4, 0, 2) (val = -0.838155), analytic = -0.058879, numerical = -0.058879, relative error = +0.000000\n",
      "OK checking param X index (4, 0, 3) (val = +1.471143), analytic = -0.071113, numerical = -0.071113, relative error = +0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK checking param X index (4, 0, 4) (val = -0.616536), analytic = +0.063540, numerical = +0.063540, relative error = +0.000000\n",
      "OK checking param X index (4, 0, 5) (val = +0.820679), analytic = -0.116850, numerical = -0.116850, relative error = +0.000000\n",
      "OK checking param X index (4, 0, 6) (val = +0.996597), analytic = -0.178264, numerical = -0.178264, relative error = +0.000000\n",
      "OK checking param X index (4, 0, 7) (val = -0.859234), analytic = +0.093350, numerical = +0.093350, relative error = +0.000000\n",
      "OK checking param X index (4, 0, 8) (val = +1.339412), analytic = +0.139525, numerical = +0.139525, relative error = +0.000000\n",
      "OK checking param X index (4, 0, 9) (val = +1.206057), analytic = +0.101469, numerical = +0.101469, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 0) (val = +0.264013), analytic = +0.024066, numerical = +0.024066, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 1) (val = +0.212939), analytic = -0.016170, numerical = -0.016170, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 2) (val = +0.914755), analytic = -0.013913, numerical = -0.013913, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 3) (val = +0.068315), analytic = -0.108872, numerical = -0.108872, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 4) (val = +0.132229), analytic = +0.002499, numerical = +0.002499, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 5) (val = +1.920657), analytic = +0.003490, numerical = +0.003490, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 6) (val = +1.680888), analytic = -0.011615, numerical = -0.011615, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 7) (val = -0.637729), analytic = +0.086836, numerical = +0.086836, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 8) (val = +1.869642), analytic = -0.119718, numerical = -0.119718, relative error = +0.000000\n",
      "OK checking param X index (4, 1, 9) (val = -1.423768), analytic = +0.011668, numerical = +0.011668, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 0) (val = -0.062018), analytic = +0.145256, numerical = +0.145256, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 1) (val = -1.088156), analytic = +0.140658, numerical = +0.140658, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 2) (val = -0.055676), analytic = -0.067784, numerical = -0.067784, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 3) (val = +0.051981), analytic = -0.196068, numerical = -0.196068, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 4) (val = -0.105556), analytic = +0.045104, numerical = +0.045104, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 5) (val = -2.765127), analytic = -0.094917, numerical = -0.094917, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 6) (val = +0.383988), analytic = -0.187330, numerical = -0.187330, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 7) (val = -1.324033), analytic = -0.044088, numerical = -0.044088, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 8) (val = -0.186684), analytic = +0.035752, numerical = +0.035752, relative error = +0.000000\n",
      "OK checking param X index (4, 2, 9) (val = +0.112910), analytic = -0.049763, numerical = -0.049763, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 0) (val = +0.000000), analytic = +0.132629, numerical = +0.132629, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 1) (val = +0.000000), analytic = +0.026617, numerical = +0.026617, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 2) (val = +0.000000), analytic = -0.223734, numerical = -0.223734, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 3) (val = +0.000000), analytic = -0.267728, numerical = -0.267728, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 4) (val = +3.000000), analytic = +0.096154, numerical = +0.096154, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 5) (val = +3.000000), analytic = -0.049906, numerical = -0.049906, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 6) (val = +3.000000), analytic = +0.086477, numerical = +0.086477, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 7) (val = +3.000000), analytic = +0.529258, numerical = +0.529258, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 8) (val = +0.000000), analytic = +1.053814, numerical = +1.053814, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 9) (val = +0.000000), analytic = -0.433660, numerical = -0.433660, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 10) (val = +0.000000), analytic = +0.099715, numerical = +0.099715, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 11) (val = +0.000000), analytic = +0.292059, numerical = +0.292059, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 12) (val = +0.000000), analytic = +0.509997, numerical = +0.509997, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 13) (val = +0.000000), analytic = -0.875973, numerical = -0.875973, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 14) (val = +0.000000), analytic = +0.328388, numerical = +0.328388, relative error = +0.000000\n",
      "OK checking param WLSTM index (0, 15) (val = +0.000000), analytic = +2.277794, numerical = +2.277794, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 0) (val = -0.103743), analytic = +0.385761, numerical = +0.385761, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 1) (val = +0.004070), analytic = -0.117020, numerical = -0.117020, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 2) (val = -0.408040), analytic = -0.025209, numerical = -0.025209, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 3) (val = -0.237489), analytic = +0.394977, numerical = +0.394977, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 4) (val = +0.033464), analytic = -0.035242, numerical = -0.035242, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 5) (val = -0.507366), analytic = +0.065903, numerical = +0.065903, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 6) (val = -0.164659), analytic = -0.036550, numerical = -0.036550, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 7) (val = +0.201710), analytic = +0.425834, numerical = +0.425834, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 8) (val = +0.566529), analytic = +0.054091, numerical = +0.054091, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 9) (val = +0.252660), analytic = +1.101552, numerical = +1.101552, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 10) (val = -0.115095), analytic = +0.945655, numerical = +0.945655, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 11) (val = +0.191594), analytic = -0.008095, numerical = -0.008095, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 12) (val = +0.301525), analytic = +0.185362, numerical = +0.185362, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 13) (val = +0.266343), analytic = +0.237599, numerical = +0.237599, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 14) (val = +0.184096), analytic = +1.035223, numerical = +1.035223, relative error = +0.000000\n",
      "OK checking param WLSTM index (1, 15) (val = +0.079416), analytic = -0.126014, numerical = -0.126014, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 0) (val = -0.040988), analytic = +0.066760, numerical = +0.066760, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 1) (val = +0.343946), analytic = +0.008470, numerical = +0.008470, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 2) (val = +0.046530), analytic = +0.130767, numerical = +0.130767, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 3) (val = -0.001724), analytic = +0.038515, numerical = +0.038515, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 4) (val = -0.458926), analytic = -0.003367, numerical = -0.003367, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 5) (val = +0.007692), analytic = +0.011610, numerical = +0.011610, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 6) (val = +0.195458), analytic = -0.021460, numerical = -0.021460, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 7) (val = -0.150720), analytic = -0.139247, numerical = -0.139247, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 8) (val = +0.446961), analytic = +0.106503, numerical = +0.106503, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 9) (val = -0.093299), analytic = +1.238657, numerical = +1.238657, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 10) (val = +0.023869), analytic = -0.260662, numerical = -0.260662, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 11) (val = -0.126346), analytic = -0.325098, numerical = -0.325098, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 12) (val = +0.114384), analytic = +0.018665, numerical = +0.018665, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 13) (val = -0.397730), analytic = +0.060531, numerical = +0.060531, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 14) (val = +0.127868), analytic = +0.589129, numerical = +0.589129, relative error = +0.000000\n",
      "OK checking param WLSTM index (2, 15) (val = -0.185610), analytic = -0.409559, numerical = -0.409559, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 0) (val = -0.452000), analytic = -0.183976, numerical = -0.183976, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 1) (val = -0.030563), analytic = +0.012819, numerical = +0.012819, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 2) (val = +0.149375), analytic = +0.044070, numerical = +0.044070, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 3) (val = +0.195833), analytic = -0.329164, numerical = -0.329164, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 4) (val = -0.162334), analytic = -0.004478, numerical = -0.004478, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 5) (val = -0.234564), analytic = -0.015365, numerical = -0.015365, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 6) (val = -0.081556), analytic = +0.016596, numerical = +0.016596, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 7) (val = +0.277023), analytic = +0.015982, numerical = +0.015982, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 8) (val = -0.102958), analytic = -0.695681, numerical = -0.695681, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 9) (val = +0.275033), analytic = +0.544525, numerical = +0.544525, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 10) (val = -0.162706), analytic = +0.093310, numerical = +0.093310, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 11) (val = -0.438041), analytic = +0.044558, numerical = +0.044558, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 12) (val = -0.177209), analytic = +0.013855, numerical = +0.013855, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 13) (val = +0.118553), analytic = +0.109591, numerical = +0.109591, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 14) (val = -0.206104), analytic = -0.839544, numerical = -0.839544, relative error = +0.000000\n",
      "OK checking param WLSTM index (3, 15) (val = -0.125640), analytic = +0.113615, numerical = +0.113615, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 0) (val = -0.163236), analytic = +0.266802, numerical = +0.266802, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 1) (val = +0.225029), analytic = -0.028331, numerical = -0.028331, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 2) (val = +0.300906), analytic = +0.073589, numerical = +0.073589, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 3) (val = +0.568927), analytic = +0.123837, numerical = +0.123837, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 4) (val = +0.151538), analytic = -0.011803, numerical = -0.011803, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 5) (val = -0.020505), analytic = -0.009173, numerical = -0.009173, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 6) (val = -0.021883), analytic = +0.046021, numerical = +0.046021, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 7) (val = +0.084280), analytic = -0.090904, numerical = -0.090904, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 8) (val = -0.597459), analytic = +0.298009, numerical = +0.298009, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 9) (val = +0.080691), analytic = -0.393469, numerical = -0.393469, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 10) (val = -0.068914), analytic = +0.145964, numerical = +0.145964, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 11) (val = -0.034745), analytic = -0.388013, numerical = -0.388013, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 12) (val = +0.137550), analytic = -0.474689, numerical = -0.474689, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 13) (val = +0.365236), analytic = -0.121426, numerical = -0.121426, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 14) (val = -0.320451), analytic = +0.263685, numerical = +0.263685, relative error = +0.000000\n",
      "OK checking param WLSTM index (4, 15) (val = -0.215678), analytic = -0.907757, numerical = -0.907757, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 0) (val = +0.085833), analytic = -0.010278, numerical = -0.010278, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 1) (val = +0.418924), analytic = +0.056515, numerical = +0.056515, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 2) (val = -0.227586), analytic = -0.127333, numerical = -0.127333, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 3) (val = -0.046844), analytic = -0.120322, numerical = -0.120322, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 4) (val = +0.499534), analytic = -0.016834, numerical = -0.016834, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 5) (val = -0.185082), analytic = +0.041033, numerical = +0.041033, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 6) (val = +0.119798), analytic = -0.062627, numerical = -0.062627, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 7) (val = -0.276788), analytic = +0.369477, numerical = +0.369477, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 8) (val = +0.177006), analytic = -0.336000, numerical = -0.336000, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 9) (val = -0.034829), analytic = +0.715072, numerical = +0.715072, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 10) (val = -0.240838), analytic = +0.023388, numerical = +0.023388, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 11) (val = -0.007621), analytic = +0.024947, numerical = +0.024947, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 12) (val = -0.064247), analytic = +0.292959, numerical = +0.292959, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 13) (val = -0.081444), analytic = +0.260135, numerical = +0.260135, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 14) (val = -0.163971), analytic = +0.486040, numerical = +0.486040, relative error = +0.000000\n",
      "OK checking param WLSTM index (5, 15) (val = +0.297395), analytic = +0.244519, numerical = +0.244519, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 0) (val = +0.018052), analytic = -0.137153, numerical = -0.137153, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 1) (val = -0.275225), analytic = +0.076536, numerical = +0.076536, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 2) (val = +0.146200), analytic = -0.180458, numerical = -0.180458, relative error = +0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK checking param WLSTM index (6, 3) (val = -0.018228), analytic = -0.416594, numerical = -0.416594, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 4) (val = -0.053925), analytic = +0.016140, numerical = +0.016140, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 5) (val = +0.346925), analytic = +0.064477, numerical = +0.064477, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 6) (val = -0.159411), analytic = -0.062510, numerical = -0.062510, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 7) (val = -0.017574), analytic = +0.561745, numerical = +0.561745, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 8) (val = -0.289963), analytic = -0.528406, numerical = -0.528406, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 9) (val = +0.330393), analytic = +1.300261, numerical = +1.300261, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 10) (val = +0.060579), analytic = -0.233715, numerical = -0.233715, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 11) (val = -0.067353), analytic = +0.272197, numerical = +0.272197, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 12) (val = +0.283115), analytic = +0.533919, numerical = +0.533919, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 13) (val = -0.138072), analytic = -0.154637, numerical = -0.154637, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 14) (val = -0.313582), analytic = +1.873991, numerical = +1.873991, relative error = +0.000000\n",
      "OK checking param WLSTM index (6, 15) (val = -0.288137), analytic = +0.561397, numerical = +0.561397, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 0) (val = -0.474142), analytic = -0.172362, numerical = -0.172362, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 1) (val = +0.373726), analytic = +0.016830, numerical = +0.016830, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 2) (val = +0.219507), analytic = -0.055273, numerical = -0.055273, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 3) (val = +0.141095), analytic = -0.354457, numerical = -0.354457, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 4) (val = +0.243658), analytic = -0.009719, numerical = -0.009719, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 5) (val = +0.092166), analytic = +0.011863, numerical = +0.011863, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 6) (val = +0.620121), analytic = -0.038502, numerical = -0.038502, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 7) (val = -0.126592), analytic = +0.008810, numerical = +0.008810, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 8) (val = -0.401354), analytic = -0.439023, numerical = -0.439023, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 9) (val = +0.531941), analytic = -0.449165, numerical = -0.449165, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 10) (val = -0.325432), analytic = +0.048037, numerical = +0.048037, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 11) (val = +0.154047), analytic = -0.250999, numerical = -0.250999, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 12) (val = -0.450061), analytic = -0.046540, numerical = -0.046540, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 13) (val = +0.423131), analytic = -0.048293, numerical = -0.048293, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 14) (val = -0.138749), analytic = -0.119133, numerical = -0.119133, relative error = +0.000000\n",
      "OK checking param WLSTM index (7, 15) (val = +0.355632), analytic = +0.415618, numerical = +0.415618, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 0) (val = +0.181551), analytic = -0.090287, numerical = -0.090287, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 1) (val = -0.127641), analytic = -0.060307, numerical = -0.060307, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 2) (val = +0.344054), analytic = +0.098412, numerical = +0.098412, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 3) (val = -0.083248), analytic = +0.395788, numerical = +0.395788, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 4) (val = +0.146692), analytic = -0.046447, numerical = -0.046447, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 5) (val = +0.368575), analytic = +0.049664, numerical = +0.049664, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 6) (val = +0.215480), analytic = -0.068452, numerical = -0.068452, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 7) (val = +0.413225), analytic = -0.202285, numerical = -0.202285, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 8) (val = -0.138856), analytic = -0.513696, numerical = -0.513696, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 9) (val = -0.037348), analytic = -0.636242, numerical = -0.636242, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 10) (val = +0.087650), analytic = +0.496221, numerical = +0.496221, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 11) (val = -0.395787), analytic = +0.177327, numerical = +0.177327, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 12) (val = +0.063184), analytic = +0.262986, numerical = +0.262986, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 13) (val = -0.206022), analytic = +0.420714, numerical = +0.420714, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 14) (val = +0.396707), analytic = +0.230847, numerical = +0.230847, relative error = +0.000000\n",
      "OK checking param WLSTM index (8, 15) (val = +0.167765), analytic = -0.917263, numerical = -0.917263, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 0) (val = +0.216210), analytic = -0.230603, numerical = -0.230603, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 1) (val = -0.164844), analytic = +0.116779, numerical = +0.116779, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 2) (val = +0.467624), analytic = -0.129452, numerical = -0.129452, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 3) (val = +0.128341), analytic = -0.654553, numerical = -0.654553, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 4) (val = -0.276905), analytic = +0.037400, numerical = +0.037400, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 5) (val = +0.104573), analytic = +0.021075, numerical = +0.021075, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 6) (val = -0.060263), analytic = -0.005870, numerical = -0.005870, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 7) (val = -0.003417), analytic = +0.267796, numerical = +0.267796, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 8) (val = -0.028425), analytic = -0.483211, numerical = -0.483211, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 9) (val = -0.476343), analytic = +0.417811, numerical = +0.417811, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 10) (val = +0.191391), analytic = +0.249289, numerical = +0.249289, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 11) (val = -0.117089), analytic = -0.148428, numerical = -0.148428, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 12) (val = -0.530404), analytic = +0.547402, numerical = +0.547402, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 13) (val = -0.306806), analytic = -0.128811, numerical = -0.128811, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 14) (val = +0.017043), analytic = +0.344752, numerical = +0.344752, relative error = +0.000000\n",
      "OK checking param WLSTM index (9, 15) (val = -0.650151), analytic = +1.238125, numerical = +1.238125, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 0) (val = -0.437581), analytic = +0.162448, numerical = +0.162448, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 1) (val = -0.070677), analytic = +0.202869, numerical = +0.202869, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 2) (val = -0.324134), analytic = -0.190430, numerical = -0.190430, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 3) (val = -0.135309), analytic = -0.272292, numerical = -0.272292, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 4) (val = -0.003826), analytic = +0.012310, numerical = +0.012310, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 5) (val = +0.029850), analytic = +0.003391, numerical = +0.003391, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 6) (val = +0.076462), analytic = +0.027086, numerical = +0.027086, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 7) (val = -0.212164), analytic = +0.796479, numerical = +0.796479, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 8) (val = +0.039793), analytic = -0.539757, numerical = -0.539757, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 9) (val = +0.052843), analytic = +0.243806, numerical = +0.243806, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 10) (val = -0.393578), analytic = -0.369934, numerical = -0.369934, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 11) (val = -0.295337), analytic = +0.316719, numerical = +0.316719, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 12) (val = -0.098537), analytic = +0.169387, numerical = +0.169387, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 13) (val = -0.322673), analytic = -0.031943, numerical = -0.031943, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 14) (val = +0.150400), analytic = -0.409360, numerical = -0.409360, relative error = +0.000000\n",
      "OK checking param WLSTM index (10, 15) (val = -0.011914), analytic = +0.552755, numerical = +0.552755, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 0) (val = +0.379325), analytic = -0.245794, numerical = -0.245794, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 1) (val = -0.292504), analytic = +0.046899, numerical = +0.046899, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 2) (val = +0.025459), analytic = -0.073104, numerical = -0.073104, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 3) (val = +0.107701), analytic = -0.160490, numerical = -0.160490, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 4) (val = +0.171554), analytic = -0.010107, numerical = -0.010107, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 5) (val = -0.254105), analytic = -0.018901, numerical = -0.018901, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 6) (val = +0.102611), analytic = -0.036764, numerical = -0.036764, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 7) (val = +0.035529), analytic = -0.275434, numerical = -0.275434, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 8) (val = +0.438817), analytic = -0.473886, numerical = -0.473886, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 9) (val = -0.111772), analytic = +0.079497, numerical = +0.079497, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 10) (val = +0.104244), analytic = -0.361122, numerical = -0.361122, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 11) (val = +0.168306), analytic = -0.127722, numerical = -0.127722, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 12) (val = +0.015120), analytic = +0.099265, numerical = +0.099265, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 13) (val = -0.316178), analytic = -0.046394, numerical = -0.046394, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 14) (val = -0.613228), analytic = -0.028230, numerical = -0.028230, relative error = +0.000000\n",
      "OK checking param WLSTM index (11, 15) (val = -0.290060), analytic = +0.302129, numerical = +0.302129, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 0) (val = +0.235263), analytic = +0.342129, numerical = +0.342129, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 1) (val = -0.349321), analytic = +0.002404, numerical = +0.002404, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 2) (val = -0.006424), analytic = -0.117876, numerical = -0.117876, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 3) (val = -0.318875), analytic = +0.166951, numerical = +0.166951, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 4) (val = +0.029276), analytic = +0.006067, numerical = +0.006067, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 5) (val = -0.171781), analytic = -0.001370, numerical = -0.001370, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 6) (val = +0.434306), analytic = +0.041599, numerical = +0.041599, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 7) (val = -0.591335), analytic = +0.664963, numerical = +0.664963, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 8) (val = +0.041487), analytic = +0.167081, numerical = +0.167081, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 9) (val = -0.634982), analytic = -0.245658, numerical = -0.245658, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 10) (val = +0.359936), analytic = +0.393954, numerical = +0.393954, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 11) (val = +0.275290), analytic = +0.252644, numerical = +0.252644, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 12) (val = +0.247910), analytic = -0.026524, numerical = -0.026524, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 13) (val = -0.073786), analytic = -0.187832, numerical = -0.187832, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 14) (val = +0.215173), analytic = -0.184941, numerical = -0.184941, relative error = +0.000000\n",
      "OK checking param WLSTM index (12, 15) (val = +0.198827), analytic = +0.337055, numerical = +0.337055, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 0) (val = -0.256193), analytic = +0.088001, numerical = +0.088001, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 1) (val = -0.358574), analytic = +0.029302, numerical = +0.029302, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 2) (val = -0.025822), analytic = -0.041433, numerical = -0.041433, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 3) (val = +0.037910), analytic = +0.091186, numerical = +0.091186, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 4) (val = +0.157044), analytic = -0.026596, numerical = -0.026596, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 5) (val = -0.005784), analytic = +0.030810, numerical = +0.030810, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 6) (val = -0.031757), analytic = -0.037597, numerical = -0.037597, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 7) (val = -0.256735), analytic = +0.319306, numerical = +0.319306, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 8) (val = +0.457528), analytic = -0.355850, numerical = -0.355850, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 9) (val = +0.053447), analytic = +0.381129, numerical = +0.381129, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 10) (val = +0.083412), analytic = +0.084375, numerical = +0.084375, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 11) (val = -0.463301), analytic = +0.124033, numerical = +0.124033, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 12) (val = +0.287561), analytic = +0.045730, numerical = +0.045730, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 13) (val = -0.078391), analytic = +0.191224, numerical = +0.191224, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 14) (val = -0.049734), analytic = +0.155898, numerical = +0.155898, relative error = +0.000000\n",
      "OK checking param WLSTM index (13, 15) (val = -0.398530), analytic = -0.356410, numerical = -0.356410, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 0) (val = -0.010323), analytic = -0.121626, numerical = -0.121626, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 1) (val = -0.064002), analytic = +0.051374, numerical = +0.051374, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 2) (val = -0.365190), analytic = -0.062192, numerical = -0.062192, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 3) (val = -0.461843), analytic = -0.140338, numerical = -0.140338, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 4) (val = -0.105193), analytic = +0.001595, numerical = +0.001595, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 5) (val = -0.299414), analytic = +0.015914, numerical = +0.015914, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 6) (val = +0.206667), analytic = -0.045216, numerical = -0.045216, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 7) (val = -0.362429), analytic = +0.056787, numerical = +0.056787, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 8) (val = +0.478278), analytic = -0.259876, numerical = -0.259876, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 9) (val = -0.067895), analytic = +0.509241, numerical = +0.509241, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 10) (val = -0.141070), analytic = -0.227822, numerical = -0.227822, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 11) (val = -0.266862), analytic = +0.089707, numerical = +0.089707, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 12) (val = +0.578402), analytic = +0.287902, numerical = +0.287902, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 13) (val = -0.021427), analytic = +0.029248, numerical = +0.029248, relative error = +0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK checking param WLSTM index (14, 14) (val = +0.135901), analytic = +0.363846, numerical = +0.363846, relative error = +0.000000\n",
      "OK checking param WLSTM index (14, 15) (val = +0.231116), analytic = +0.373330, numerical = +0.373330, relative error = +0.000000\n",
      "OK checking param c0 index (0, 0) (val = +1.131881), analytic = +0.135576, numerical = +0.135576, relative error = +0.000000\n",
      "OK checking param c0 index (0, 1) (val = +1.695387), analytic = -0.004428, numerical = -0.004428, relative error = +0.000000\n",
      "OK checking param c0 index (0, 2) (val = +0.156207), analytic = +0.490657, numerical = +0.490657, relative error = +0.000000\n",
      "OK checking param c0 index (0, 3) (val = +0.703295), analytic = -0.423202, numerical = -0.423202, relative error = +0.000000\n",
      "OK checking param c0 index (1, 0) (val = +0.251690), analytic = +0.829503, numerical = +0.829503, relative error = +0.000000\n",
      "OK checking param c0 index (1, 1) (val = -0.387842), analytic = -0.214127, numerical = -0.214127, relative error = +0.000000\n",
      "OK checking param c0 index (1, 2) (val = -0.789222), analytic = -0.309060, numerical = -0.309060, relative error = +0.000000\n",
      "OK checking param c0 index (1, 3) (val = +1.239179), analytic = +1.176687, numerical = +1.176687, relative error = +0.000000\n",
      "OK checking param c0 index (2, 0) (val = -0.292481), analytic = -0.231093, numerical = -0.231093, relative error = +0.000000\n",
      "OK checking param c0 index (2, 1) (val = +1.170708), analytic = -0.134743, numerical = -0.134743, relative error = +0.000000\n",
      "OK checking param c0 index (2, 2) (val = -1.234994), analytic = -0.197468, numerical = -0.197468, relative error = +0.000000\n",
      "OK checking param c0 index (2, 3) (val = -0.593017), analytic = +0.475464, numerical = +0.475464, relative error = +0.000000\n",
      "OK checking param h0 index (0, 0) (val = -1.174874), analytic = +0.108294, numerical = +0.108294, relative error = +0.000000\n",
      "OK checking param h0 index (0, 1) (val = -0.063991), analytic = +0.008294, numerical = +0.008294, relative error = +0.000000\n",
      "OK checking param h0 index (0, 2) (val = -0.337909), analytic = +0.268270, numerical = +0.268270, relative error = +0.000000\n",
      "OK checking param h0 index (0, 3) (val = -0.356935), analytic = +0.147669, numerical = +0.147669, relative error = +0.000000\n",
      "OK checking param h0 index (1, 0) (val = -1.069916), analytic = +0.037561, numerical = +0.037561, relative error = +0.000000\n",
      "OK checking param h0 index (1, 1) (val = +2.104492), analytic = -0.017223, numerical = -0.017223, relative error = +0.000000\n",
      "OK checking param h0 index (1, 2) (val = +1.003246), analytic = -0.275287, numerical = -0.275287, relative error = +0.000000\n",
      "OK checking param h0 index (1, 3) (val = -0.131926), analytic = -0.040888, numerical = -0.040888, relative error = +0.000000\n",
      "OK checking param h0 index (2, 0) (val = +0.162150), analytic = +0.004968, numerical = +0.004968, relative error = +0.000000\n",
      "OK checking param h0 index (2, 1) (val = -0.494642), analytic = -0.096198, numerical = -0.096198, relative error = +0.000000\n",
      "OK checking param h0 index (2, 2) (val = -0.971814), analytic = -0.054779, numerical = -0.054779, relative error = +0.000000\n",
      "OK checking param h0 index (2, 3) (val = -0.178396), analytic = +0.138820, numerical = +0.138820, relative error = +0.000000\n",
      "every line should start with OK. Have a nice day!\n"
     ]
    }
   ],
   "source": [
    "checkSequentialMatchesBatch()\n",
    "# raw_input('check OK, press key to continue to gradient check')\n",
    "print ('should all be True')\n",
    "checkBatchGradient()\n",
    "print ('every line should start with OK. Have a nice day!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por agregar lectura "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, agrega en esta misma celda comentarios sobre la comparación entre los dos modelos vistos, diferencias, similitudes, ventajas, desventajas. Recuerda es una opinión personal basada en tu trabajo, no pongas lo que dice la literatura si no lo que tu experimentaste a la hora de desarrollar y aplicar los modelos, así no concuerde con lo que leas en otros lados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
